---
sidebar_position: 14
---

import {
  LangChainInstallationCodeTabs,
  LangChainQuickStartCodeTabs,
} from "@site/src/components/QuickStart";

import {
  CodeTabs,
  TypeScriptBlock,
  PythonBlock,
  typescript,
  python,
} from "@site/src/components/InstructionsWithCode";

# Trace with `LangChain` (Python and JS/TS)

LangSmith integrates seamlessly with LangChain ([Python](https://python.langchain.com/) and [JS](https://js.langchain.com/docs/get_started/introduction)), the popular open-source framework for building LLM applications.

## Installation

Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).

For a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).

<LangChainInstallationCodeTabs />

## Quick start

### 1. Configure your environment

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>

# The below examples use the OpenAI API, so you will need
export OPENAI_API_KEY=<your-openai-api-key>
```

### 2. Log a trace

No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.

<LangChainQuickStartCodeTabs />

### 3. View your trace

By default, the trace will be logged to the project with the name `default`. An example of a trace logged using the above code is made public and can be viewed [here](https://smith.langchain.com/public/e6a46eb2-d785-4804-a1e3-23f167a04300/r).

![](../static/langchain_trace.png)

## Trace selectively

The [previous section](#quick-start) showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application.

There are two ways to do this in Python: by manually passing in a `LangChainTracer` ([reference docs](https://api.python.langchain.com/en/latest/tracers/langchain_core.tracers.langchain.LangChainTracer.html#langchain_core.tracers.langchain.LangChainTracer)) instance as a callback, or by using the `tracing_v2_enabled` context manager ([reference docs](https://api.python.langchain.com/en/latest/tracers/langchain_core.tracers.context.tracing_v2_enabled.html)).

In JS/TS, you can pass a `LangChainTracer` ([reference docs](https://api.js.langchain.com/classes/langchain_core_tracers_tracer_langchain.LangChainTracer.html)) instance as a callback.

<CodeTabs
  tabs={[
    PythonBlock(`# You can configure a LangChainTracer instance to trace a specific invocation.
from langchain.callbacks.tracers import LangChainTracer\n
tracer = LangChainTracer()
chain.invoke({"question": "Am I using a callback?", "context": "I'm using a callback"}, config={"callbacks": [tracer]})\n
# LangChain Python also supports a context manager for tracing a specific block of code.
from langchain_core.tracers.context import tracing_v2_enabled
with tracing_v2_enabled():
    chain.invoke({"question": "Am I using a context manager?", "context": "I'm using a context manager"})\n
# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)
chain.invoke({"question": "Am I being traced?", "context": "I'm not being traced"})`),
    TypeScriptBlock(`// You can configure a LangChainTracer instance to trace a specific invocation.
import { LangChainTracer } from "@langchain/core/tracers/tracer_langchain";\n
const tracer = new LangChainTracer();
await chain.invoke(
  {
    question: "Am I using a callback?",
    context: "I'm using a callback"
  },
  { callbacks: [tracer] }
);`),
  ]}
  groupId="client-language"
/>

## Log to a specific project

### Statically

As mentioned in the [tracing conceptual guide](../../concepts/tracing) LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the `LANGCHAIN_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.

```shell
export LANGCHAIN_PROJECT=my-project
```

### Dynamically

This largely builds off of the [previous section](#trace-selectively) and allows you to set the project name for a specific `LangChainTracer` instance or as parameters to the `tracing_v2_enabled` context manager in Python.

<CodeTabs
  tabs={[
    PythonBlock(`# You can set the project name for a specific tracer instance:
from langchain.callbacks.tracers import LangChainTracer\n
tracer = LangChainTracer(project_name="My Project")
chain.invoke({"question": "Am I using a callback?", "context": "I'm using a callback"}, config={"callbacks": [tracer]})\n
# You can set the project name using the project_name parameter.
from langchain_core.tracers.context import tracing_v2_enabled
with tracing_v2_enabled(project_name="My Project"):
    chain.invoke({"question": "Am I using a context manager?", "context": "I'm using a context manager"})`),
    TypeScriptBlock(`// You can set the project name for a specific tracer instance:
import { LangChainTracer } from "@langchain/core/tracers/tracer_langchain";\n
const tracer = new LangChainTracer({ projectName: "My Project" });
await chain.invoke(
  {
    question: "Am I using a callback?",
    context: "I'm using a callback"
  },
  { callbacks: [tracer] }
);`),
  ]}
  groupId="client-language"
/>

## Add metadata and tags to traces

LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see [this guide](./export_traces)

<CodeTabs
  tabs={[
    PythonBlock(`from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser\n
prompt = ChatPromptTemplate.from_messages([
  ("system", "You are a helpful AI."),
  ("user", "{input}")
])
chat_model = ChatOpenAI()
output_parser = StrOutputParser()\n
# highlight-next-line
# Tags and metadata can be configured with RunnableConfig
chain = (prompt | chat_model | output_parser).with_config({"tags": ["top-level-tag"], "metadata": {"top-level-key": "top-level-value"}})\n
# highlight-next-line
# Tags and metadata can also be passed at runtime
chain.invoke({"input": "What is the meaning of life?"}, {"tags": ["shared-tags"], "metadata": {"shared-key": "shared-value"}})`),
    TypeScriptBlock(`import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";\n
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful AI."],
  ["user", "{input}"]
])
const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });
const outputParser = new StringOutputParser();\n
// highlight-next-line
// Tags and metadata can be configured with RunnableConfig
const chain = (prompt.pipe(model).pipe(outputParser)).withConfig({"tags": ["top-level-tag"], "metadata": {"top-level-key": "top-level-value"}});\n
// highlight-next-line
// Tags and metadata can also be passed at runtime
await chain.invoke({input: "What is the meaning of life?"}, {tags: ["shared-tags"], metadata: {"shared-key": "shared-value"}})`),
  ]}
  groupId="client-language"
/>

## Customize run name

When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.

:::note
This feature is not currently supported directly for LLM objects.
:::

<CodeTabs
  tabs={[
    PythonBlock(`# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').
configured_chain = chain.with_config({"run_name": "MyCustomChain"})
configured_chain.invoke({"query": "What is the meaning of life?"})`),
    TypeScriptBlock(`// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').
// (Note: this is not currently supported directly on LLM objects.)
...
const configuredChain = chain.withConfig({ runName: "MyCustomChain" });
await configuredChain.invoke({query: "What is the meaning of life?"});`),
  ]}
  groupId="client-language"
/>

## Access run (span) ID for LangChain invocations

When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith.

In Python, you can use the `collect_runs` context manager to access the run ID.

In JS/TS, you can use a `RunCollectorCallbackHandler` instance to access the run ID.

<CodeTabs
  tabs={[
    PythonBlock(`from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tracers.context import collect_runs\n
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Please respond to the user's request only based on the given context."),
    ("user", "Question: {question}\\n\\nContext: {context}")
])
model = ChatOpenAI(model="gpt-3.5-turbo")
output_parser = StrOutputParser()\n
chain = prompt | model | output_parser\n
question = "Can you summarize this morning's meetings?"
context = "During this morning's meeting, we solved all world conflict."
with collect_runs() as cb:
  result = chain.invoke({"question": question, "context": context})
  # Get the root run id
  # highlight-next-line
  run_id = cb.traced_runs[0].id
print(run_id)`),
    TypeScriptBlock(`import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { RunCollectorCallbackHandler } from "@langchain/core/tracers/run_collector";\n
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant. Please respond to the user's request only based on the given context."],
  ["user", "Question: {question\\n\\nContext: {context}"],
]);
const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });
const outputParser = new StringOutputParser();\n
const chain = prompt.pipe(model).pipe(outputParser);
const runCollector = new RunCollectorCallbackHandler();\n
const question = "Can you summarize this morning's meetings?"
const context = "During this morning's meeting, we solved all world conflict."
await chain.invoke(
    { question: question, context: context },
    { callbacks: [runCollector] }
);
// highlight-next-line
const runId = runCollector.tracedRuns[0].id;
console.log(runId);`),
  ]}
  groupId="client-language"
/>

## Ensure all traces are submitted before exiting

In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.

In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"true"`.

For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain_openai import ChatOpenAI
from langchain_core.tracers.langchain import wait_for_all_tracers\n
llm = ChatOpenAI()
try:
    llm.invoke("Hello, World!")
finally:
    # highlight-next-line
    wait_for_all_tracers()
`),
    TypeScriptBlock(`import { ChatOpenAI } from "@langchain/openai";
import { awaitAllCallbacks } from "@langchain/core/callbacks/promises";\n
try {
  const llm = new ChatOpenAI();
  const response = await llm.invoke("Hello, World!");
} catch (e) {
  // handle error
} finally {
   // highlight-next-line
  await awaitAllCallbacks();
}
`),
  ]}
  groupId="client-language"
/>

## Trace without setting environment variables

As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:

- `LANGCHAIN_TRACING_V2`
- `LANGCHAIN_API_KEY`
- `LANGCHAIN_ENDPOINT`
- `LANGCHAIN_PROJECT`

However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.

This largely builds off of the [previous section](#trace-selectively).

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.callbacks.tracers import LangChainTracer
from langsmith import Client\n
# You can create a client instance with an api key and api url
client = Client(
    api_key="YOUR_API_KEY",  # This can be retrieved from a secrets manager
    api_url="https://api.smith.langchain.com",  # Update appropriately for self-hosted installations
)\n
# You can pass the client and project_name to the LangChainTracer instance
# highlight-next-line
tracer = LangChainTracer(client=client, project_name="test-no-env")
chain.invoke({"question": "Am I using a callback?", "context": "I'm using a callback"}, config={"callbacks": [tracer]})\n
# LangChain Python also supports a context manager which allows passing the client and project_name
from langchain_core.tracers.context import tracing_v2_enabled
# highlight-next-line
with tracing_v2_enabled(client=client, project_name="test-no-env"):
    chain.invoke({"question": "Am I using a context manager?", "context": "I'm using a context manager"})`),
    TypeScriptBlock(`import { LangChainTracer } from "@langchain/core/tracers/tracer_langchain";
import { Client } from "langsmith";\n
// You can create a client instance with an api key and api url
const client = new Client(
    {
        apiKey: "YOUR_API_KEY",
        apiUrl: "https://api.smith.langchain.com",
    }
);\n
// You can pass the client and project_name to the LangChainTracer instance
// highlight-next-line
const tracer = new LangChainTracer({client, projectName: "test-no-env"});
await chain.invoke(
  {
    question: "Am I using a callback?",
    context: "I'm using a callback",
  },
  { callbacks: [tracer] }
);`),
  ]}
  groupId="client-language"
/>

## Interoperability between LangChain.JS and LangSmith SDK

### Tracing LangChain objects inside `traceable` (JS only)

Starting with `langchain@0.2.x`, LangChain objects are traced automatically when used inside `@traceable` functions, inheriting the client, tags, metadata and project name of the traceable function.

For older versions of LangChain, you will need to manually pass an instance `LangChainTracer` created from the tracing context found in `@traceable`.

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
// highlight-next-line
import { getLangchainCallbacks } from "langsmith/langchain";

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant. Please respond to the user's request only based on the given context.",
  ],
  ["user", "Question: {question}\nContext: {context}"],
]);
const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });
const outputParser = new StringOutputParser();

const chain = prompt.pipe(model).pipe(outputParser);

const main = traceable(
  async (input: { question: string; context: string }) => {
    // this is done automatically in langchain@0.2.x
    // only do this if you cannot upgrade your LangChain version
    // highlight-start
    const callbacks = await getLangchainCallbacks();
    const response = await chain.invoke({ callbacks });
    // highlight-end
    return response;
  },
  { name: "main" }
);
```

### Manually tracing LangChain child runs via `traceable` / RunTree API (JS only)

In some uses cases, you might want to run `traceable` functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the `RunTree` API.

You can convert the existing LangChain `RunnableConfig` to a equivalent RunTree object by using `RunTree.fromRunnableConfig` or you can pass the `RunnableConfig` as the first argument of the wrapped function.

<CodeTabs
  tabs={[
    typescript({ value: "traceable", label: "Traceable" })`
      import { DynamicStructuredTool } from "@langchain/core/tools";
      import { RunTree } from "langsmith/run_trees";
      import { traceable } from "langsmith/traceable";
      import { z } from "zod";
      
      const tracedChild = traceable((input: string) => input, {
        name: "Child Run",
      });
      
      const tool = new DynamicStructuredTool({
        name: "Search",
        description: "Search the web for information.",
        schema: z.object({ text: z.string() }),
        func: async ({ text }, manager, config) => {
          // Pass the config to existing traceable function
          // highlight-next-line
          return await tracedChild(config, "Hello");
        },
      });
    `,
    typescript({ value: "runtree", label: "Run Tree" })`
      import { DynamicStructuredTool } from "@langchain/core/tools";
      import { RunTree } from "langsmith/run_trees";
      import { traceable } from "langsmith/traceable";
      import { z } from "zod";
      
      const tool = new DynamicStructuredTool({
        name: "Search",
        description: "Search the web for information.",
        schema: z.object({ text: z.string() }),
        func: async ({ text }, manager, config) => {
          
          // create a new run tree using the existing runnable config
          // highlight-start
          const childRunTree = RunTree.fromRunnableConfig(config, {
            name: "Child Run",
          });
          // highlight-end
          
          childRunTree.inputs = { text };
          await childRunTree.postRun();
          
          let response = "Hello";
          
          childRunTree.outputs = { output: "Hello" };
          await childRunTree.patchRun();
        },
      });
    `,
  ]}
