---
sidebar_position: 17
---

import {
  CodeTabs,
  typescript,
  python,
} from "@site/src/components/InstructionsWithCode";
import { RegionalUrl } from "@site/src/components/RegionalUrls";

# Trace without setting environment variables

As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:

- `LANGCHAIN_TRACING_V2`
- `LANGCHAIN_API_KEY`
- `LANGCHAIN_ENDPOINT`
- `LANGCHAIN_PROJECT`

In some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.

<CodeTabs
  tabs={[
    python({
      caption:
        "The recommended way to do this in Python is to use the [`trace` context manager](./annotate_code#use-the-trace-context-manager-python-only) and pass in a configured `langsmith.Client` object.",
    })`
        import openai
        from langsmith import trace
        from langsmith import Client, traceable
        from langsmith.wrappers import wrap_openai
        
        langsmith_client = Client(
            api_key="YOUR_API_KEY",  # This can be retrieved from a secrets manager
            api_url="https://api.smith.langchain.com",  # Update appropriately for self-hosted installations or the EU region
        )
        
        client = wrap_openai(openai.Client())
        
        @traceable(run_type="tool", name="Retrieve Context")
        def my_tool(question: str) -> str:
            return "During this morning's meeting, we solved all world conflict."
        
        def chat_pipeline(question: str):
            context = my_tool(question)
            messages = [
                { "role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context." },
                { "role": "user", "content": f"Question: {question}\nContext: {context}"}
            ]
            chat_completion = client.chat.completions.create(
                model="gpt-3.5-turbo", messages=messages
            )
            return chat_completion.choices[0].message.content
        
        app_inputs = {"input": "Can you summarize this morning's meetings?"}
        
        # highlight-next-line
        with trace("Chat Pipeline", "chain", project_name="test-no-env", inputs=app_inputs, client=langsmith_client) as rt:
            output = chat_pipeline("Can you summarize this morning's meetings?")
            rt.end(outputs={"output": output})
    `,
    typescript({
      caption:
        "In TypeScript, you can pass in both the client and the `tracingEnabled` flag to the `traceable` decorator.",
    })`
        import { Client } from "langsmith";
        import { traceable } from "langsmith/traceable";
        import { wrapOpenAI } from "langsmith/wrappers";
        import { OpenAI } from "openai";
        
        const client = new Client({
            apiKey: "YOUR_API_KEY",  // This can be retrieved from a secrets manager
            apiUrl: "https://api.smith.langchain.com",  // Update appropriately for self-hosted installations or the EU region
        });
        
        const openai = wrapOpenAI(new OpenAI());
        
        const tool = traceable((question: string) => {
            return "During this morning's meeting, we solved all world conflict.";
        }, { name: "Retrieve Context", runType: "tool" });
        
        const pipeline = traceable(
            async (question: string) => {
                const context = await tool(question);
                
                const completion = await openai.chat.completions.create({
                    model: "gpt-3.5-turbo",
                    messages: [
                        { role: "system" as const, content: "You are a helpful assistant. Please respond to the user's request only based on the given context." },
                        { role: "user" as const, content: \`Question: \${question}\\nContext: \${context}\`}
                    ]
                });
                
                return completion.choices[0].message.content;
            
            }, 
            // highlight-start
            { name: "Chat", client, tracingEnabled: true }
            // highlight-end
        );
        
        await pipeline("Can you summarize this morning's meetings?");
    `,
  ]}
/>
