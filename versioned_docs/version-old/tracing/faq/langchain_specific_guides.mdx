---
sidebar_label: LangChain-Specific Guides
sidebar_position: 6
---

import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
  LangChainPyBlock,
  LangChainJSBlock,
  APIBlock,
} from "@site/src/components/InstructionsWithCode";

import { AccessRunIdBlock } from "@site/src/components/TracingFaq";

# LangChain-specific guides

This sections contains guides that are specific to users using LangChain (both Python and JavaScript).

### Grouping runs from multi-turn interactions

When using LangChain, you may want to group runs from multi-turn interactions together.

With chatbots, copilots, and other common LLM design patterns, users frequently interact with your model over multiple interactions. Each invocation of your model is logged as a separate trace, but you can group these traces together using metadata (see [how to add metadata to a run](#how-do-i-add-metadata-or-tags-to-a-run) above for more information).
Below is a minimal example with LangChain, but **the same idea applies when using the LangSmith SDK or API.**

<CodeTabs
  tabs={[
    LangChainPyBlock(`from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI\n
chain = (
    ChatPromptTemplate.from_messages(
        [
            ("system", "You are a helpful AI."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{message}"),
        ]
    )
    | ChatOpenAI(model="gpt-3.5-turbo")
    | StrOutputParser()
)\n
conversation_id = "101e8e66-9c68-4858-a1b4-3b0e3c51a933"\n
chat_history = []
message = HumanMessage(content="Hi there")
response = ""
for chunk in chain.stream(
    {
        "message": message,
        "chat_history": chat_history,
    },
    # highlight-next-line
    config={"metadata": {"conversation_id": conversation_id}},
):
    print(chunk, end="")
    response += chunk
print()
chat_history.extend(
    [
        message,
        AIMessage(content=response),
    ]
)\n
# ... Next message comes in
next_message = HumanMessage(content="I don't need much assistance, actually.")
for chunk in chain.stream(
    {
        "message": next_message,
        "chat_history": chat_history,
    },
    # highlight-next-line
    config={"metadata": {"conversation_id": conversation_id}},
):
    print(chunk, end="")
    response += chunk
`),
    LangChainJSBlock(`import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";\n
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful AI."],
  ["user", "{message}"],
]);
const chain = prompt
  .pipe(new ChatOpenAI({ model: "gpt-3.5-turbo" }))
  .pipe(new StringOutputParser());\n
const conversationId = "101e8e66-9c68-4858-a1b4-3b0e3c51a933";
const chatHistory = [];
const message = { content: "Hi there" };
let response = "";
for await (const chunk of await chain.stream(
  {
    message,
  },
  {
    // highlight-next-line
    metadata: { conversation_id: conversationId },
  }
)) {
  process.stdout.write(chunk);
  response += chunk;
}
console.log();
chatHistory.push(message, { content: response });\n
// ... Next message comes in
const nextMessage = { content: "I don't need much assistance, actually." };
for await (const chunk of await chain.stream(
  {
    message: nextMessage,
  },
  {
    // highlight-next-line
    metadata: { conversation_id: conversationId },
  }
)) {
  process.stdout.write(chunk);
  response += chunk;
}`),
  ]}
  groupId="client-language"
/>

To view all the traces from that conversation in LangSmith, you can query the project using a metadata filter:

```bash
has(metadata, '{"conversation_id":"101e8e66-9c68-4858-a1b4-3b0e3c51a933"}')
```

This will return all traces with the specified conversation ID.

### Getting a run ID from a LangChain call

In Typescript, the run ID is returned in the call response under the `__run` key. In python, we recommend using the run collector callback.
Below is an example:

<AccessRunIdBlock />

If using the API or SDK, you can send the run ID as a parameter to `RunTree` or the `traceable` decorator.

### Tracing without environment variables

Some situations don't permit the use of environment variables or don't expose `process.env`. This is mostly pertinent when running LangChain apps in certain JavaScript runtime environments. To add tracing in these situations, you can manually create the `LangChainTracer` callback and pass it to the chain, LLM, or other LangChain component, either when initializing or in the call itself. This is the same tactic used for [changing the tracer project](#how-do-i-change-the-tracer-project) within a program.

Example:

<CodeTabs
  tabs={[
    LangChainPyBlock(`from langchain.callbacks import LangChainTracer
from langchain_openai import ChatOpenAI
from langsmith import Client\n
callbacks = [
  LangChainTracer(
    project_name="YOUR_PROJECT_NAME_HERE",
    client=Client(
      api_url="https://api.smith.langchain.com",
      api_key="YOUR_API_KEY_HERE"
    )
  )
]\n
llm = ChatOpenAI()
llm.invoke("Hello, world!", config={"callbacks": callbacks})
`),
    LangChainJSBlock(`import { Client } from "langsmith";
import { LangChainTracer } from "langchain/callbacks";
import { ChatOpenAI } from "langchain/chat_models/openai";\n
const callbacks = [
  new LangChainTracer({
    projectName: "YOUR_PROJECT_NAME_HERE",
    client: new Client({
      apiUrl: "https://api.smith.langchain.com",
      apiKey: "YOUR_API_KEY_HERE",
    }),
  }),
];
const llm = new ChatOpenAI({});
await llm.invoke("Hello, world!", { callbacks });
`),
  ]}
  groupId="client-language"
/>

This tactic is also useful for when you have multiple chains running in a shared environment but want to log their run traces to different projects.

### Ensuring all traces are submitted before exiting

In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.

In LangChain JS, prior to `@langchain/core` version `0.3.0`, the default was to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. Versions `>=0.3.0` will have the same default as Python.
You can explicitly make callbacks synchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"false"` or asynchronous by setting it to `"true"`. You can also check out [this guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more options for awaiting backgrounded callbacks in serverless environments.

For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example:

<CodeTabs
  tabs={[
    LangChainPyBlock(`from langchain_openai import ChatOpenAI
from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n
llm = ChatOpenAI()
try:
    llm.invoke("Hello, World!")
finally:
    # highlight-next-line
    wait_for_all_tracers()
`),
    LangChainJSBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { awaitAllCallbacks } from "langchain/callbacks";\n
try {
  const llm = new ChatOpenAI();
  const response = await llm.invoke("Hello, World!");
} catch (e) {
  // handle error
} finally {
  await awaitAllCallbacks();
}
`),
  ]}
  groupId="client-language"
/>
