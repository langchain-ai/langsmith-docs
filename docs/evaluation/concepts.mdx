---
sidebar_label: Concepts
sidebar_position: 2
---


# Concepts

In this guide we will go over some of the import concepts to know when evaluating your application.

## Examples

At the heart of evaluation lies an example.
An example is a particular datapoint to evaluate a chain, agent, or model on.
An example contains the inputs and (optionally) expected outputs for a given interaction.

## Datasets

Datasets are collections of examples.
All examples in the dataset should follow the same schema.

There are a few different types of datasets. Dataset types communicate common input and output schemas. For almost all use cases, the default "kv" dataset type is sufficient. The other two types, "llm" and "chat", can be useful to conveniently export datasets into known fine-tuning formats.


### Key-value datasets

Datasets with the data type "kv" are the default type. For these datasets, inputs and outputs can be arbitrary key-value pairs. These are useful when evaluating chains and agents that require multiple inputs or that return multiple outputs.

The tradeoff with these datasets is that running evaluations on them can be a bit more involved. If there are multiple keys, you will have to manually specify the `prepare_data` function in any off-the-shelf evaluators so they know what information to consider in generating a score.

### LLM datasets

Datasets with the "llm" data type correspond to the string inpts and outputs from the "completion" style LLMS (string in, string out).

The `"inputs"` dictionary contains a single `"input"` key mapped to a single prompt string. Similarly, the `"outputs"` dictionary contains a single `"output"` key mapped to a single response string.


### Chat datasets

Datasets with the "chat" data type correspond to messages and generations from LLMs that expect structured "chat" messages as inputs and outputs. Each example row
expects an `"inputs"` dictionary containing a single `"input"` key mapped to a list of serialized chat messages. The `"outputs"` dictionary contains a single `"output"` key mapped to a single list of serialized chat messages.

## `llm_or_chain_factory`

In order to evaluate your application, you will want to pass a parameter `llm_or_chain_factory`.
Despite the intimidating name, this is a relatively simple concept.

This just refers to the application or logic you wish to evaluate.
This could be a single LLM, a prompt + LLM, a complex chain, an agent, or anything really!
This can be built using LangChain or without LangChain.
In order to evaluate this system you will run examples through this system and observe the output.

The reason this is called a `factory` is that sometimes chains, agents, and application are stateful.
When running over multiple datapoints, we want them to be stateless.
Therefore, if your application is stateful you should pass in a "factory" which constructs the chain or agent.

## Evaluators

Evaluators are functions that help score how well your system did on a particular example.
They take in two things:

1. An example (containing inputs for your pipeline and optionally the reference outputs or labels)
2. Observed output (gathered from running the inputs through the system to evaluate)

An evaluator will then return an `EvaluationResult` (or similarly shaped dictionary).

#### Run Object

The key `Run` object fields are as follows:

- `outputs`: `Dict[str, Any]` - The outputs of your pipeline
- `inputs`: `Dict[str, Any]` - The inputs to your pipeline. These are the same as the inputs in the example.
- `child_runs`: `List[Run]` - If your pipeline has nested steps, these can be accessed and used in your evaluator

#### Example Object

The key `Example` object fields are as follows:
- `outputs`: `Dict[str, Any]` - The reference labels or other context found in your dataset
- `inputs`: `Dict[str, Any]` - The inputs to your pipeline
- `metadata`: `Dict[str, Any]` - Any other metadata you have stored in that example within the dataset

## EvaluationResult

An `EvaluationResult` contains the result of an evaluator on a particular datapoint.
This is made up of:

- `key`: The name the metric being evaluated
- `score`: The value of the metric on this example
- `comment`: the reasoning trajectory or other string information motivating the score


## Experiment Prefix

Each experiment has a name - this is usually set via the `experiment_prefix`. This should be an interpretable name that helps you understand what the experiment is testing.

The test runner will automatically add a unique identifier to the end of the experiment name to make it unique.

A common suggestion for a name is something like `ClaudeWithRRF`.

This contains an understandable start to the name, but also a random uuid to make it unique.

