---
sidebar_label: How to create custom evaluators
sidebar_position: 1
---

# Evaluators

In this guide, you will create custom evaluators to grade your LLM system. An evaluator can apply any logic you want, returning a numeric `score` associated with a `key`.

Most evaluators are applied on a `run` level, scoring each prediction individually.
Some `summary_evaluators` can be applied on a `experiment` level, letting you score and aggregate metrics across multiple runs.

## Run Evaluators

Run evaluators are applied to each prediction of your pipeline. The common automated evaluator types are:

1. Simple Heuristics: Checking for regex matches, presence/absence of certain words or code, etc.
2. AI-assisted: Instruct an "LLM-as-judge" to grade the output of a run based on the prediction and reference answer (or retrieved context).

We will demonstrate some simple ones below.

#### Example 1: Exact Match Evaluator

Let's start with a simple evaluator that checks if the model's output exactly matches the reference answer.

```python
from langsmith.schemas import Example, Run

def exact_match(run: Run, example: Example) -> dict:
    reference = example.outputs["answer"]
    prediction = run.outputs["output"]
    score = prediction.lower() == reference.lower()
    return {"key": "exact_match", "score": score}
```

Let's break this down:

- The evaluator function accepts a `Run` and `Example` and returns a dictionary with the evaluation key and score. The run contains the full trace of your pipeline, and the example contains the inputs and outputs for this data point. If your dataset contains labels, they are found in the `example.outputs` dictionary, which is kept separate to keep your model from cheating.
- In our dataset, the outputs have an "answer" key that contains the reference answer. Your pipeline generates predictions as a dictionary with an "output" key.
- It compares the prediction and reference (case-insensitive) and returns a dictionary with the evaluation key and score.

You can use this evaluator directly in the `evaluate` function:

```python
from langsmith.evaluation import evaluate

evaluate(
    <your prediction function>,
    data="<dataset_name>",
    evaluators=[exact_match],
)
```

#### Example 2: Parametrizing your evaluator

You may want to parametrize your evaluator as a class. This is useful when you need to pass additional parameters to the evaluator.

```python
from langsmith.evaluation import evaluate
from langsmith.schemas import Example, Run

class BlocklistEvaluator:
    def __init__(self, blocklist: list[str]):
        self.blocklist = blocklist
    def __call__(
        self, run: Run, example: Example | None = None
    ) -> dict:
        model_outputs = run.outputs["output"]
        score = not any([word in model_outputs for word in self.blocklist])
        return {"key": "blocklist", "score": score}


evaluate(
    <your prediction function>,
    data="<dataset_name>",
    evaluators=[BlocklistEvaluator(blocklist=["bad", "words"])],
)
```

#### Example 3: Evaluating nested traces

While most evaluations are applied to the inputs and outputs of your system, you can also evaluate all of the subcomponents that are traced within your pipeline.

This is possible by stepping through the `run` object and collecting the outputs of each component.

As a simple example, let's assume you want to evaluate the expected tools that are invoked in a pipeline.

```python
from langsmith.evaluation import evaluate
from langsmith.schemas import Example, Run

def evaluate_trajectory(run: Run, example: Example) -> dict:
    # collect the tools on level 1 of the trace tree
    steps = [child.name for child in run.child_runs if child.run_type == "tool"]
    expected_steps = example.outputs["expected_tools"]
    score = len(set(steps) & set(expected_steps)) / len(set(steps) | set(expected_steps))
    return {"key": "tools", "score": score}
```

This assumes tools are properly typed in the trace tree.

#### Example 3: Structured Output

With function calling, it has become easier than ever to generate feedback metrics using LLMs as a judge simply by specifying a Pydantic schema for the output.

Below is an example (in this case using LangChain's `with_structured_output` functionality) to evaluate RAG app faithfulness.

```python
from typing import List
from langsmith.evaluation import evaluate
from langsmith.schemas import Example, Run
from langchain_openai import ChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field

class Propositions(BaseModel):
    propositions: List[str] = Field(description="The factual propositions generated by the model")

class FaithfulnessScore(BaseModel):
    reasoning: str = Field(description="The reasoning for the faithfulness score")
    score: bool

extractor = ChatOpenAI(model="gpt-4-turbo-preview").with_structured_output(Propositions)
grader = ChatOpenAI(model="gpt-4-turbo-preview").with_structured_output(FaithfulnessScore)

def faithfulness(run: Run, example: Example) -> dict:
    # Assumes your RAG app includes the prediction in the "output" key in its response
    response: str = run.outputs["output"]
    # Assumes your RAG app includes the retrieved docs as a "context" key in the outputs
    retrieved_docs: list = run.outputs["context"]
    formatted_docs = "\n".join([str(doc) for doc in retrieved_docs])
    extracted = extractor.invoke(response)
    scores = grader.batch(extracted.propositions)
    scores = [score.score for score in scores]
    average_score = sum(scores) / len(scores)
    comment = "\n".join([score.reasoning for score in scores])
    return {"key": "faithfulness", "score": average_score, "comment": comment}
```

#### Example 4: Perplexity Evaluator

The flexibility of the functional interface means you can easly apply evaluators from any other libraries. For instance, you may want to use statistical measures such as [`perplexity`](https://huggingface.co/spaces/evaluate-metric/perplexity) to grade your run output. Below is an example using the [evaluate](https://huggingface.co/docs/evaluate/index) package by HuggingFace, which contains numerous commonly used metrics. Start by installing the `evaluate` package by running `pip install evaluate`.

```python
from evaluate import load
from langsmith.schemas import Example, Run

class PerplexityEvaluator:
    def __init__(self, prediction_key: Optional[str] = None, model_id: str = "gpt-2"):
        self.prediction_key = prediction_key
        self.model_id = model_id
        self.metric_fn = load("perplexity", module_type="metric")
    def evaluate_run(
        self, run: Run, example: Example
    ) -> dict:
        if run.outputs is None:
            raise ValueError("Run outputs cannot be None")
        prediction = run.outputs[self.prediction_key]
        results = self.metric_fn.compute(
            predictions=[prediction], model_id=self.model_id
        )
        ppl = results["perplexities"][0]
        return {"key": "Perplexity", "score": ppl}
```

Let's break down what the `PerplexityEvaluator` is doing:

- **Initialize**: In the constructor, we're setting up a few properties that will be needed later on.
  - `prediction_key`: The key to find the model's prediction in the outputs of a run.
  - `model_id`: The ID of the language model you want to use to compute the metric. In our example, we are using 'gpt-2'.
  - `metric_fn`: The evaluation metric function, loaded from the HuggingFace `evaluate` package.
- **Evaluate**: This method takes a run (and optionally an example) and returns an evaluation dictionary.
  - If the run outputs are `None`, the evaluator raises an error.
  - Otherwise, the outputs are passed to the `metric_fn` to compute the perplexity. The perplexity score is then returned as part of the evaluation dictionary.
    Once you've defined your evaluators, you can use them to evaluate your model:

```python
from langsmith.evaluation import evaluate
evaluate(
    <LLM or chain or agent>,
    data="<dataset_name>",
    evaluators=[BlocklistEvaluator(blocklist=["bad", "words"]), PerplexityEvaluator(), is_empty],
)
```

## Summary Evaluators

Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the f1 score of a classifier across all runs in an experiment kicked off from a dataset. These are called `summary_evaluators`.

```python
from typing import List
from langsmith.schemas import Example, Run
from langsmith.evaluation import evaluate

def f1_score_summary_evaluator(examples: List[Example]) -> dict:
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    for example in examples:
        reference = example.outputs["answer"]
        prediction = example.outputs["output"]
        if reference and prediction == reference:
            true_positives += 1
        elif prediction and not reference:
            false_positives += 1
        elif not prediction and reference:
            false_negatives += 1
    if true_positives == 0:
        return {"key": "f1_score", "score": 0.0}

    precision = true_positives / (true_positives + false_positives)
    recall = true_positives / (true_positives + false_negatives)
    f1_score = 2 * (precision * recall) / (precision + recall)
    return {"key": "f1_score", "score": f1_score}

# Assumes the dataset's outputs has an "answer" key that is a booleans
evaluate(
    lambda x: {"output": True}, # Your classifier
    data="<dataset_name>",
    summary_evaluators=[f1_score_summary_evaluator],
)
```

### Recap

Congratulations! You created a custom evaluation chain you can apply to _any_ traced run so you can surface more relevant information in your application.
LangChain's evaluation chains speed up the development process for application-specific, semantically robust evaluations.
You can also extend existing components from the library so you can focus on building your product. All your evals come with:

- Automatic tracing integrations to help you debug, compare, and improve your code
- Easy sharing and mixing of components and results
- Out-of-the-box support for sync and async evaluations for faster runs
