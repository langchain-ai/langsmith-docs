import {
  python,
} from "@site/src/components/InstructionsWithCode";

# Evaluate applications with large file inputs

Attachments allow you to associate large files with your examples. This allows you to evaluate RAG applications
over large internal documents, benchmark image analysis tools, etc.

## Create a dataset with attachments

Attachments allow you to host large files outside of your inputs/outputs, which means faster loading times as well
as nicer rendering of files in the LangSmith UI. You can think of it like attaching a file to an email instead 
of copy pasting the entire contents into your email body.
To create a dataset with attachments, you need to use the `upsert_examples_multipart` method of the LangSmith client:

```python
from langsmith import Client
from langsmith.schemas import ExampleUpsertWithAttachments

# Define the LANGCHAIN_API_KEY environment variable with your api key
langsmith_client = Client()

dataset = langsmith_client.create_dataset(
    dataset_name="attachment-test-dataset",
    description="Test dataset for evals with attachments",
)

# Define the example
example = ExampleUpsertWithAttachments(
    dataset_id=dataset.id,
    inputs={"question": "What were the cumulative earnings earned from online orders in the midwest during Q2?"},
    outputs={"answer": "$123456"},
    attachments={
        # Each attachments is just a name with a mime type and the path or bytes content of the file
        "pdf": ("application/pdf", Path("./foo_earnings.pdf").read_bytes()),
        # We can pass multiple attachments (of different types!), as long as they have different names
        "pptx": ("application/pptx", Path("./foo_earnings.pptx").read_bytes()),
        # Can also just pass in the path
        "txt": ("text/plain", Path("./foo_earnings.txt")),
    },
)

# Upsert the examples
langsmith_client.upsert_examples_multipart(upserts=[example])
```

## Define a target function with attachments

Now that we have a dataset that includes examples with attachments, we can define a target function to run our LLM application with the attachments.
The target function must have two positional arguments, the first must be called `inputs` and the second must be called `attachments`.

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
import fitz
from io import BytesIO
import base64

model = ChatOpenAI(model="gpt-4o-mini")

def pdf_to_image_bytes(pdf_bytes, image_format='PNG'):
    pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
    images = []
    for page in pdf_document:
        pix = page.get_pixmap()
        img_bytes = BytesIO()
        pix.pil_save(img_bytes, format=image_format)
        # Encode the bytes in base64
        base64_bytes = base64.b64encode(img_bytes.getvalue()).decode('utf-8')
        images.append(base64_bytes)
    pdf_document.close()
    return images


def pdf_qa(inputs, attachments):
    system_message = SystemMessage(
        content="The images are of the pdf that the question is referencing. Use the images to generate your answer."
    )
    # The attachment tuple returned contains the S3 url first and then a reader of the bytes
    pdf_s3_url, pdf_reader = attachments['pdf']
    images = pdf_to_image_bytes(pdf_reader.read())
    human_message = HumanMessage(
        content=[
            {"type": "text", "text": inputs["question"]},
        ] + [{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image}"}} for image in images],
    )
    messages = [system_message, human_message]
    return {"answer": model.invoke(messages).content}
```

## Run an evaluation

We can then run an evaluation as usual, by passing in the target function to the `evaluate` method:

```python
# We can optionally define an evaluator to use
def evaluator(outputs: dict, reference_outputs: dict):
    score = int(str(outputs["answer"]) in reference_outputs["answer"])
    return {"key": "correctness", "score": score}

evaluate(
    target=pdf_qa,
    data="attachment-test-dataset",
    client=langsmith_client,
)
```