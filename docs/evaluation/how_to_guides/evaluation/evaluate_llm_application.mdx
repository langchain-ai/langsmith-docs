---
sidebar_position: 1
---

import {
  CodeTabs,
  python,
  typescript,
  PythonBlock,
  TypeScriptBlock,
} from "@site/src/components/InstructionsWithCode";

# How to run an evaluation

:::tip Recommended reading
Before diving into this content, it might be helpful to read the following:

- [Conceptual guide on evaluation](../../concepts)
- [How-to guide on managing datasets](../datasets/manage_datasets_in_application)
- [How-to guide on managing datasets programmatically](../datasets/manage_datasets_programmatically)

:::

Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application.

## Step 1: Define your application logic

In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline.

To understand how to annotate your code for tracing, please refer to [this guide](../../../observability/how_to_guides/tracing/annotate_code).

<CodeTabs
  groupId="client-language"
  tabs={[
    python`
      from langsmith import traceable, wrappers
      from openai import Client
      
      client = wrappers.wrap_openai(Client())
      
      @traceable
      def label_text(text):
          system = (
            "Please review the user query below and determine if it contains any form of toxic behavior, "
            "such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, "
            "and 'Not toxic' if it doesn't."
          )
          messages = [
              {"role": "system", "content": system},
              {"role": "user", "content": text},
          ]
          result = client.chat.completions.create(
              messages=messages, model="gpt-4o-mini", temperature=0
          )
          return result.choices[0].message.content
      `,
    typescript`
      import { OpenAI } from "openai";
      import { wrapOpenAI } from "langsmith/wrappers";
      import { traceable } from "langsmith/traceable";
      
      const client = wrapOpenAI(new OpenAI());
      
      const labelText = traceable(
        async (text: string) => {
          const result = await client.chat.completions.create({
            messages: [
              { 
                role: "system",
                content: "Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",
              },
              { role: "user", content: text },
            ],
            model: "gpt-4o-mini",
            temperature: 0,
          });
          
          return result.choices[0].message.content;
        },
        { name: "labelText" }
      );`,
  ]}
/>

## Step 2: Create or select a dataset

In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text.

Each `Example` in the dataset contains three dictionaries / objects:

- `outputs`: The reference labels or other context found in your dataset
- `inputs`: The inputs to your pipeline
- `metadata`: Any other metadata you have stored in that example within the dataset

These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings.

<CodeTabs
  groupId="client-language"
  tabs={[
    python`
      from langsmith import Client
      
      client = Client()
      
      # Create a dataset
      examples = [
          ("Shut up, idiot", "Toxic"),
          ("You're a wonderful person", "Not toxic"),
          ("This is the worst thing ever", "Toxic"),
          ("I had a great day today", "Not toxic"),
          ("Nobody likes you", "Toxic"),
          ("This is unacceptable. I want to speak to the manager.", "Not toxic"),
      ]
      
      dataset_name = "Toxic Queries"
      dataset = client.create_dataset(dataset_name=dataset_name)
      inputs, outputs = zip(
          *[({"text": text}, {"label": label}) for text, label in examples]
      )
      client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)
    `,
    typescript`
      import { Client } from "langsmith";
      
      const langsmith = new Client();
      
      // create a dataset
      const toxicExamples = [
        ["Shut up, idiot", "Toxic"],
        ["You're a wonderful person", "Not toxic"],
        ["This is the worst thing ever", "Toxic"],
        ["I had a great day today", "Not toxic"],
        ["Nobody likes you", "Toxic"],
        ["This is unacceptable. I want to speak to the manager.", "Not toxic"],
      ];
      
      const [inputs, outputs] = toxicExamples.reduce<
        [Array<{ input: string }>, Array<{ outputs: string }>]
      >(
        ([inputs, outputs], item) => [
          [...inputs, { input: item[0] }],
          [...outputs, { outputs: item[1] }],
        ],
        [[], []]
      );
      
      const datasetName = "Toxic Queries";
      const toxicDataset = await langsmith.createDataset(datasetName);
      await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });
    `,
  ]}
/>

## Step 3. Configure evaluators to score the outputs

In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the [following section](#custom-evaluators).

<CodeTabs
  groupId="client-language"
  tabs={[
    python`
      from langsmith.schemas import Example, Run

      def correct_label(root_run: Run, example: Example) -> dict:
          score = root_run.outputs.get("output") == example.outputs.get("label")
          return {"score": int(score), "key": "correct_label"}
    `,
    typescript`
      import type { EvaluationResult } from "langsmith/evaluation";
      import type { Run, Example } from "langsmith/schemas";
      
      // Row-level evaluator
      function correctLabel(rootRun: Run, example: Example): EvaluationResult {
        const score = rootRun.outputs?.outputs === example.outputs?.output;
        return { key: "correct_label", score };
      }
    `,
  ]}
/>

## Step 4. Run the evaluation and view the results

You can use the `evaluate` method in Python and TypeScript to run an evaluation.

At its simplest, the `evaluate` method takes the following arguments:

- a function that takes an input dictionary or object and returns an output dictionary or object
- `data` - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples
- `evaluators` - a list of evaluators to score the outputs of the function
- `experiment_prefix` - a string to prefix the experiment name with. A name will be generated if not provided.

<CodeTabs
  groupId="client-language"
  tabs={[
    python`
      from langsmith.evaluation import evaluate
      
      dataset_name = "Toxic Queries"
      
      results = evaluate(
          lambda inputs: label_text(inputs["text"]),
          data=dataset_name,
          evaluators=[correct_label],
          experiment_prefix="Toxic Queries",
          description="Testing the baseline system.",  # optional
      )
    `,
    typescript`
      import { evaluate } from "langsmith/evaluation";
      
      const datasetName = "Toxic Queries";
      
      await evaluate((inputs) => labelText(inputs["input"]), {
        data: datasetName,
        evaluators: [correctLabel],
        experimentPrefix: "Toxic Queries",
      });
    `,
  ]}
/>

Each invocation of `evaluate` produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator.

_If you've annotated your code for tracing, you can open the trace of each row in a side panel view._

![](../evaluation/static/view_experiment.gif)

