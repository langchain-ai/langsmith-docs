---
sidebar_position: 1
---

import {
  CodeTabs,
  python,
  typescript,
} from "@site/src/components/InstructionsWithCode";

# Log user feedback using the SDK

:::tip Key concepts

- [Conceptual guide on tracing and feedback](../../../observability/concepts)
- [Reference guide on feedback data format](/reference/data_formats/feedback_data_format)

:::

LangSmith makes it easy to attach feedback to traces. 
This feedback can come from users, annotators, automated evaluators, etc., and is crucial for monitoring and evaluating applications.

## Use [create_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Here we'll walk through how to log feedback using the SDK.

:::info Child runs

You can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.
This is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.

:::

:::tip Non-blocking creation (Python only)

The Python client will automatically background feedback creation if you pass `trace_id=` to [create_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback).
This is essential for low-latency environments, where you want to make sure your application isn't blocked on feedback creation.

:::

<CodeTabs
  tabs={[
    python({caption: "Requires `langsmith >= 0.3.43`"})
`
      from langsmith import trace, traceable, Client

      @traceable
      def foo(x):
          return {"y": x * 2}

      @traceable
      def bar(y):
          return {"z": y - 1}

      client = Client()

      inputs = {"x": 1}
      with trace(name="foobar", inputs=inputs) as root_run:
          result = foo(**inputs)
          result = bar(**result)
          root_run.outputs = result
          trace_id = root_run.id
          child_runs = root_run.child_runs

      # Provide feedback for a trace (a.k.a. a root run)
      client.create_feedback(
          key="user_feedback",
          score=1,
          trace_id=trace_id,
          comment="the user said that ..."
      )

      # Provide feedback for a child run
      foo_run_id = [run for run in child_runs if run.name == "foo"][0].id
      client.create_feedback(
          key="correctness",
          score=0,
          run_id=foo_run_id,
          # trace_id= is optional but recommended to enable batched and backgrounded 
          # feedback ingestion.
          trace_id=trace_id,
      )
    `,
    typescript({})`import { Client } from "langsmith";
const client = new Client();\n
// ... Run your application and get the run_id...
// This information can be the result of a user-facing feedback form\n
await client.createFeedback(
    runId,
    "feedback-key",
    {
        score: 1.0,
        comment: "comment",
    }
);`,
  ]}
  groupId="client-language"
/>

You can even log feedback for in-progress runs using `create_feedback() / createFeedback()`. See [this guide](../../../observability/how_to_guides/access_current_span) for how to get the run ID of an in-progress run.

To learn more about how to filter traces based on various attributes, including user feedback, see [this guide](../../../observability/how_to_guides/filter_traces_in_application).
