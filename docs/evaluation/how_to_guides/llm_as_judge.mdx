
import {
  CodeTabs,
  python,
  typescript,
} from "@site/src/components/InstructionsWithCode";

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import '@site/src/css/custom.css';


# How to define an LLM-as-a-judge evaluator

:::info Key concepts
- [LLM-as-a-judge evaluator](/evaluation/concepts#llm-as-judge)
:::

LLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.

This guide shows you how to define a LLM-as-a-judge evaluator for [offline evaluation](/evaluation/concepts#offline-evaluation) using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to [setting up online evaluations](/observability/how_to_guides/online_evaluations#configure-llm-as-judge-evaluators). 

<Tabs className="interface-tabs">
  <TabItem value="sdk" label="SDK">

<div id="sdk-content">
## Pre-built evaluators

Pre-built evaluators are a useful starting point for setting up evaluations. Refer to [pre-built evaluators](./prebuilt_evaluators) for how to use pre-built evaluators with LangSmith.

## Create your own LLM-as-a-judge evaluator

For complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) / [TypeScript](https://docs.smith.langchain.com/reference/js)).

<CodeTabs
  groupId="client-language"
  tabs={[
    python({caption: "Requires `langsmith>=0.2.0`"})`
      from langsmith import evaluate, traceable, wrappers, Client
      from openai import OpenAI
      # Assumes you've installed pydantic
      from pydantic import BaseModel
      
      # Optionally wrap the OpenAI client to trace all model calls.
      oai_client = wrappers.wrap_openai(OpenAI())
        
      def valid_reasoning(inputs: dict, outputs: dict) -> bool:
        """Use an LLM to judge if the reasoning and the answer are consistent."""

        instructions = """\\

Given the following question, answer, and reasoning, determine if the reasoning \\
for the answer is logically valid and consistent with question and the answer.\\
"""

        class Response(BaseModel):
          reasoning_is_valid: bool

        msg = f"Question: {inputs['question']}\\nAnswer: {outputs['answer']}\\nReasoning: {outputs['reasoning']}"
        response = oai_client.beta.chat.completions.parse(
          model="gpt-4o",
          messages=[{"role": "system", "content": instructions,}, {"role": "user", "content": msg}],
          response_format=Response
        )
        return response.choices[0].message.parsed.reasoning_is_valid

      # Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.
      @traceable
      def dummy_app(inputs: dict) -> dict:
        return {"answer": "hmm i'm not sure", "reasoning": "i didn't understand the question"}

      ls_client = Client()
      dataset = ls_client.create_dataset("big questions")
      examples = [
        {"inputs": {"question": "how will the universe end"}},
        {"inputs": {"question": "are we alone"}},
      ]
      ls_client.create_examples(dataset_id=dataset.id, examples=examples)

      results = evaluate(
        dummy_app,
        data=dataset,
        evaluators=[valid_reasoning]
      )
    `,

]}
/>

See [here](./custom_evaluator) for more on how to write a custom evaluator.
</div>

  </TabItem >
  <TabItem value="ui" label="UI">
  <div id="ui-content">

  ## Pre-built evaluators
  
  Pre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:
  
  - **Hallucination**: Detect factually incorrect outputs.
  - **Correctness**: Check semantic similarity to a reference.
  - **Conciseness**: Evaluate whether an answer is a concise response to a question. 
  - **Code checker**: Verify correctness of code answers.
  
  You can configure these evaluators::
  - When running an evaluation using the [playground](/prompt_engineering/concepts#prompt-playground)
  - As part of a dataset to [automatically run experiments over a dataset](/evaluation/how_to_guides/bind_evaluator_to_dataset)
  - When running an [online evaluation](/observability/how_to_guides/online_evaluations#configure-llm-as-judge-evaluators)
  
  ## Customize your LLM-as-a-judge evaluator

  By default, pre-built evaluators assume that you are running the evaluation over the full input, output and (if applicable) reference output. If your application and dataset call for specific instructions or formatting, you can customize the default prompt and which parts of the input/output/reference output should be passed to the judge LLM. You can also configure the judge LLM.
  
  ![](./static/playground_evaluator.gif)
  
 ### Select/create the evaluator
  - In the playground or from a dataset: Select the **+Evaluator** button
  - From a tracing project: Select **Add rules**, configure your rule and select **Apply evaluator**
  
  Select the **Create your own evaluator option**. Alternativley, you may start by selecting a pre-built evaluator and editing it. 
  
### Configure the evaluator

Edit the prompt, a model, set of criteria to evaluate the on, and set a feedback schema for the evaluator. 

#### Prompt

Create a new prompt, or choose an existing prompt from the [prompt hub](/prompt_engineering/quickstarts/quickstart_ui).

- **Create your own prompt**: Create a custom prompt inline. 

- **Pull a prompt from the prompt hub**: Use the **Select a prompt** dropdown to select from an existing prompt. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.

#### Model

Select the desired model from the provided options.

#### Mapping variables

Use variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, a sample example (or run) is provided for reference. Use the dropdown to select the parts of the run that are relevant for the evalaution.

You may edit/remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don't need a reference output so you may remove that variable. 

#### Preview

Previewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.

#### Improve your evaluator with few-shot examples
To better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect [human corrections](/evaluation/how_to_guides/create_few_shot_evaluators#make-corrections) on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.

If you create an evaluator with few-shot examples, LangSmith will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections.

Note that this is only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. Few-shot examples are not currently supported in evaluators that use the prompt hub.

<details>
  <summary>Learn more</summary>
  Each few-shot example will be formatted according to the configuration specified, and automatically inserted into your main prompt using the `{{Few-shot examples}} `template variable. 
  
  When configuring variable mapping for few-shot examples, it should contain the same variables as your main prompt, plus a `few_shot_explanation` and a `score` variable which should have the same name as your feedback key. For example, if your main prompt has variables `question` and `response`, and your evaluator outputs a `correctness` score, then your few-shot prompt should have `question`, `response`, `few_shot_explanation`, and `correctness`.
  
  You may also specify the number of few-shot examples to use. The default is 5. If your examples are very long, you may want to set this number lower to save tokens - whereas if your examples tend to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you.
  
  If you're using few-shot examples, LangSmith has a feature in beta that allows the most relevant examples to be dynamically selected at runtime using [dynamic few-shot examples](/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection). 
</details>

#### Feedback configuration

Feedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as [feedback](/observability/concepts#feedback) to a run or example. Defining feedback for your evaluator:

1. **Name the feedback key**: This is the name that will appear when viewing evaluation results. Names should be unique across experiments. 

2. **Add a description**: Describe what the feedback represents. 

3. **Choose a feedback type**: 
- **Boolean**: True/false feedback.
- **Categorical**: Select from predefined categories. 
- **Continuous**: Numerical scoring within a specified range.

If you are using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.

### Save the evaluator
Once your are finished configuring, save your changes. 
</div>
  </TabItem>
</Tabs>