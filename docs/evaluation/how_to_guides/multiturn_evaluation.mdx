# How to set up a multiturn evaluation

LangSmith makes it easy to evaluate multiturn conversations in the playground. This allows you to evaluate how changing your system prompt,
the tools available to the model, or the output schema affects a conversation with multiple messages.

This how-to guide walks you through the various ways you can set up the playground for multiturn evaluation, depending on where your
conversation data is stored.

## From an existing run

First, ensure you have properly [traced](../../observability) a multiturn conversation, and then navigate to your tracing
project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:

![](./static/multiturn_from_run.gif)

You can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multiturn conversation changes.

## From a dataset

Before starting, make sure you have [set up your dataset](../../evaluation/how_to_guides/manage_datasets_in_application). Since you want to evaluate
multiturn conversations, make sure there is a key in your inputs that contains a list of messages.

Once you have created your dataset, head to the playground and [load your dataset](../../prompt_engineering/how_to_guides/testing_over_dataset) to evaluate.

Then, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:

![](./static/multiturn_from_dataset.gif)

When you run your prompt, the messages from each example will be added as a list in place of the 'Messages List' variable.

## Manually

There are two ways to manually create multiturn conversations. The first way is by simply appending messages to the prompt:

![](./static/multiturn_manual.gif)

This is helpful for quick iteration, but is rigid since the multiturn conversation is hardcoded.
Instead, if you want your prompt to work with any multiturn conversation you can add a 'Messages List' variable and add your
multiturn conversation there:

![](./static/multiturn_manual_list.gif)

This allows you to just tweak the system prompt or the tools, while allowing any multiturn conversation to take the place
of the `Messages List` variable, allowing you to reuse this prompt across various evaluations.

## Next Steps

Now that you know how to set up your multi-turn evaluation, you can either do "vibe" evaluations by manually 
inspecting and judging the outputs, or you can [add evaluators](../how_to_guides#define-an-evaluator) to get quantitative results.

You can also read [these how-to guides](../../prompt_engineering/how_to_guides#playground) to learn more about how to use the playground to run evaluations.