# How-to guides

Step-by-step guides that cover key tasks and operations in LangSmith.

## Evaluation

Evaluate your LLM applications to measure their performance over time.

- [Evaluate an LLM application](./how_to_guides/evaluation/evaluate_llm_application)
  - [Run an evaluation](./how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation)
  - [Use custom evaluators](./how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators)
  - [Evaluate on a particular version of a dataset](./how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset)
  - [Evaluate on a subset of a dataset](./how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset)
  - [Evaluate on a dataset split](./how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split)
  - [Evaluate on a dataset with repetitions](./how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions)
  - [Use a summary evaluator](./how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator)
  - [Evaluate a LangChain runnable](./how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)
  - [Return multiple scores](./how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores)
- [Bind an evaluator to a dataset in the UI](./how_to_guides/evaluation/bind_evaluator_to_dataset)
- [Run an evaluation from the prompt playground](./how_to_guides/evaluation/run_evaluation_from_prompt_playground)
- [Evaluate on intermediate steps](./how_to_guides/evaluation/evaluate_on_intermediate_steps)
- [Use LangChain off-the-shelf evaluators (Python only)](./how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators)
  - [Use question and answer (correctness) evaluators](./how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators)
  - [Use criteria evaluators](./how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators)
  - [Use labeled criteria evaluators](./how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators)
  - [Use string or embedding distance metrics](./how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics)
  - [Use a custom LLM in off-the-shelf evaluators](./how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators)
  - [Handle multiple input or output fields](./how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields)
- [Compare experiment results](./how_to_guides/evaluation/compare_experiment_results)
  - [Open the comparison view](./how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view)
  - [View regressions and improvements](./how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements)
  - [Filter on regressions or improvements](./how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements)
  - [Update baseline experiment](./how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment)
  - [Select feedback key](./how_to_guides/evaluation/compare_experiment_results#select-feedback-key)
  - [Open a trace](./how_to_guides/evaluation/compare_experiment_results#open-a-trace)
  - [Expand detailed view](./how_to_guides/evaluation/compare_experiment_results#expand-detailed-view)
  - [Update display settings](./how_to_guides/evaluation/compare_experiment_results#update-display-settings)
- [Filter experiments in the UI](./how_to_guides/evaluation/filter_experiments_ui)
- [Evaluate an existing experiment](./how_to_guides/evaluation/evaluate_existing_experiment)
- [Unit test LLM applications (Python only)](./how_to_guides/evaluation/unit_testing)
- [Run pairwise evaluations](./how_to_guides/evaluation/evaluate_pairwise)
  - [Use the `evaluate_comparative` function](./how_to_guides/evaluation/evaluate_pairwise#use-the-evaluate_comparative-function)
  - [Configure inputs and outputs for pairwise evaluators](./how_to_guides/evaluation/evaluate_pairwise#configure-inputs-and-outputs-for-pairwise-evaluators)
  - [Compare two experiments with LLM-based pairwise evaluators](./how_to_guides/evaluation/evaluate_pairwise#compare-two-experiments-with-llm-based-pairwise-evaluators)
  - [View pairwise experiments](./how_to_guides/evaluation/evaluate_pairwise#view-pairwise-experiments)
- [Audit evaluator scores](./how_to_guides/evaluation/audit_evaluator_scores)
  - [In the comparison view](./how_to_guides/evaluation/audit_evaluator_scores#in-the-comparison-view)
  - [In the runs table](./how_to_guides/evaluation/audit_evaluator_scores#in-the-runs-table)
  - [In the SDK](./how_to_guides/evaluation/audit_evaluator_scores#in-the-sdk)
- [Create few-shot evaluators](./how_to_guides/evaluation/create_few_shot_evaluators)
  - [Create your evaluator](./how_to_guides/evaluation/create_few_shot_evaluators#create-your-evaluator)
  - [Make corrections](./how_to_guides/evaluation/create_few_shot_evaluators#make-corrections)
  - [View your corrections dataset](./how_to_guides/evaluation/create_few_shot_evaluators#view-your-corrections-dataset)
- [Fetch performance metrics for an experiment](./how_to_guides/evaluation/fetch_perf_metrics_experiment)
- [Run evals using the API only](./how_to_guides/evaluation/run_evals_api_only)
  - [Create a dataset](./how_to_guides/evaluation/run_evals_api_only#create-a-dataset)
  - [Run a single experiment](./how_to_guides/evaluation/run_evals_api_only#run-a-single-experiment)
  - [Run a pairwise experiment](./how_to_guides/evaluation/run_evals_api_only#run-a-pairwise-experiment)
- [Upload experiments run outside of LangSmith with the REST API](./how_to_guides/evaluation/upload_existing_experiments)
  - [Request body schema](./how_to_guides/evaluation/upload_existing_experiments#request-body-schema)
  - [Considerations](./how_to_guides/evaluation/upload_existing_experiments#considerations)
  - [Example request](./how_to_guides/evaluation/upload_existing_experiments#example-request)
  - [View the experiment in the UI](./how_to_guides/evaluation/upload_existing_experiments#view-the-experiment-in-the-ui)

## Human feedback

Collect human feedback to improve your LLM applications.

- [Capture user feedback from your application to traces](./how_to_guides/human_feedback/attach_user_feedback)
- [Set up a new feedback criteria](./how_to_guides/human_feedback/set_up_feedback_criteria)
- [Annotate traces inline](./how_to_guides/human_feedback/annotate_traces_inline)
- [Use annotation queues](./how_to_guides/human_feedback/annotation_queues)
  - [Create an annotation queue](./how_to_guides/human_feedback/annotation_queues#create-an-annotation-queue)
  - [Assign runs to an annotation queue](./how_to_guides/human_feedback/annotation_queues#assign-runs-to-an-annotation-queue)
  - [Review runs in an annotation queue](./how_to_guides/human_feedback/annotation_queues#review-runs-in-an-annotation-queue)