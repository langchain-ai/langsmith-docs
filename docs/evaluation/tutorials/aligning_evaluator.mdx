# Improving LLM-as-judge evaluators using human feedback (beta)

:::tip Recommended Reading
Before diving into this content, it might be helpful to read the following:
- [Evaluation concepts](../concepts#evaluators)
- [Creating LLM-as-a-judge evaluators](/evaluation/how_to_guides/llm_as_judge)
:::

LLM-as-a-judge evaluators are a common and effective way to evaluate AI applications. It's important to trust your evaluators because they are cruitial to helping you inform application decisions (eg. prompt, model, architecture, etc.). However, creating a reliable prompt for your LLM-as-a-judge evaluator can be tricky. 

This guide will walk you through **aligning your LLM-as-a-judge evaluator using human feedback**, ultimately improving your evaluator's quality to help you build reliable AI applications.

## How it works 
LangSmith's **Align Evaluator** feature has a series of steps that help you align your LLM-as-a-judge evaluator with human expert feedback:

1. **Select one or more experiments** that contain outputs from your application
2. Add the selected experiments to an **annotation queue** to label the data. 
3. **Test your evaluator prompt** against the labeled examples. Check the cases where your evaluator result is not aligned with the labeled data. This indicates areas where your evaluator prompt needs improvement. 
4. **Refine and repeat** to improve evaluator alignment. Update your evaluator prompt and test again. 

:::note Offline evalutors support only
Currently this feature is only supported for offline evalutors, however we do plan to support this for online evaluators as well. 
:::

## Prerequisites

In order to get started, you need a [dataset](/evaluation/concepts#datasets) with at least one [experiment](/evaluation/concepts#experiment). 
- You can upload or create a dataset using the [SDK](/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) or the [UI](/evaluation/how_to_guides/manage_datasets_in_application#set-up-your-dataset)
- You can run an experiment using the [SDK](/evaluation/how_to_guides/evaluate_llm_application#run-the-evaluation) or in the [Playground](/evaluation?mode=ui#5-run-your-evaluation)

:::tip Collecting a representative dataset
We recommend that your dataset is representative of inputs and outputs you expect to see in production.

While you don’t need to cover every possible scenario, it’s important to include examples across the full range of expected use cases. For example, if you're building a sports bot that answers questions about baseball, basketball, and football, your dataset should include at least one labeled example from each sport.
:::

![Experiment](./static/alignment_starting_point.png) 


## Getting started

#### Create a new evaluator using labeled data:

1. Navigate to the **Datasets and Experiments** tab
2. Select a dataset
3. Click on **+Evaluator**
4. Choose **Create from labeled data**
5. **Name your feedback key.** This name should reflect what you're evaluating (eg. correctness, hallucination, etc.). It is not possible to change the name of an evaluator after it is created.

#### Align an existing evaluator with experiment data:

1. Navigate to the **Datasets and Experiments** tab
2. Select a dataset
3. Go to the **Evaluators** tab
4. Click on the evaluator you want to align
5. In the "Align Evaluator with experiment data" box, click **Select Experiments**

:::note
Please note that this feature is only supported for **boolean** evaluators created after July 10th, 2025. 
:::

## 1. Select experiments

Select one or more experiments to send for human labeling. This will add all runs from the selected experiments to an [annotation queue](/evaluation/concepts#annotation-queues). 

![Add to evaluator queue](./static/add_to_evaluator_queue.gif)

To add any new experiments to an existing evaluator annotation queue, head to the **Evaluators** tab, select the evaluator you are aligning and click **Add to Queue.**

## 2. Label examples

Label examples in the annotation queue by adding a feedback score. Once you've labeled an example, click **Add to Reference Dataset**. 

:::tip Labeling examples
If you have a large number of examples in your experiments, you don't need to label every example to get started. We recommend starting with at least 20 examples, you can always add more later. We recccomend that the examples that you label are diverse and representative of your dataset to ensure that you're building a well rounded evaluator prompt. 
:::

## 3. Test your evaluator prompt against the labeled examples.

Once you have labeled traces, the next step is iterating on your evaluator prompt to mimic the labeled data as well as possible. This iteration is done in the **Evaluator Playground**. 

To go to the evaluator playground: 
- Click the **View evaluator** button on the top right of the evaluator queue. This will take you to the detail page of the evaluator you are aligning. Click the **Evaluator Playground** button to access the playground.

![Evaluator Playground](./static/evaluator_pg.gif)

In the evaluator playground you can create or edit your evaluator prompt, run it over the set of labeled examples that you created in Step 2 by clicking **Start Alignment**. Using the resulting scores, you can compare the human scores to the scores outputted by your evaluator. The alignment score is calculated by doing an equality comparison on the feedback scores given by the human expert and the LLM as judge.

![Evaluator Playground](./static/alignment_evaluator_pg.gif)

## 4. Repeat to improve evaluator alignment
Update your prompt and test again to improve evaluator alignment. 

:::tip
Updates to your evaluator prompt are not saved by default. We reccomend saving your evaluator prompt regularly, and especially after you see your alignment score improve.
:::

### Tips for improving alignment score

Improving the alignment score of your evaluator isn't an exact science but there are a few strategies we have found to be generally helpful in increasing the alignment score.

1. Identify misaligned examples

Combing through the misaligned examples and trying to group them into common failure modes is a great first step for improving your evaluator prompt. There are an endless list of
potential failure modes. Maybe the LLM is unaware of some specific acronym/term used in the misaligned examples? Maybe the LLM is making assumptions about what good/bad is? etc.
A great way to get insight into what the LLM is thinking is to enable reasoning for your LLM, so instead of your judge just outputting a score you also see the logic.

2. Explain the failure modes to the LLM

Once you have identified the common failure modes, you should put instructions in your prompt so the LLM knows how to avoid them! For example, you could explain that "MFA stands for
multi-factor authentication" if you notice it not understanding that specific acronym. Or you could tell it that "a good response will always contain at least 3 potential hotels to book"
if it is confused on what good/bad means in your evaluator context.

3. Watch out for "chance alignments"

Just because the LLM was aligned with a human expert does NOT mean the logic it used was correct. For example, the LLM could have assigned a score of 1 because of reason X, 
while the human expert assigned a score of 1 because of reason Y. If your prompt doesn't mention reason Y as a correctness condition you could be fooled into thinking your LLM
is aligned when in reality it just got lucky. Be sure to examine aligned examples as well as misaligned examples to ensure your LLM judge is behaving as expected.







