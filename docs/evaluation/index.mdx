---
sidebar_label: Quick Start
sidebar_position: 1
table_of_contents: true
---

import {
  CodeTabs,
  python,
  typescript,
  ShellBlock,
} from "@site/src/components/InstructionsWithCode";
import { RegionalUrl } from "@site/src/components/RegionalUrls";

# Evaluation quick start

This quick start will get you up and running with our evaluation SDK and Experiments UI.

## 1. Install LangSmith

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "bash",
      content: `pip install -U langsmith`,
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "bash",
      content: `yarn add langsmith`,
    },
  ]}
  groupId="client-language"
/>

## 2. Create an API key

To create an API key head to the <RegionalUrl text='Settings page' suffix='/settings' />. Then click **Create API Key.**

## 3. Set up your environment

<CodeTabs
  tabs={[
    ShellBlock(`export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>`),
  ]}
  groupId="client-language"
/>

## 4. Run your evaluation

<CodeTabs
  tabs={[
    python({caption: "Requires `langsmith>=0.1.145`"})`
    from langsmith import evaluate, Client

    # 1. Create and/or select your dataset
    client = Client()
    dataset = client.clone_public_dataset(
        "https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d"
    )

    # 2. Define an evaluator
    def is_concise(outputs: dict, reference_outputs: dict) -> bool:
        return len(outputs["answer"]) < (3 * len(reference_outputs["answer"]))

    # 3. Define the interface to your app
    def chatbot(inputs: dict) -> dict:
        return {"answer": inputs["question"] + " is a good question. I don't know the answer."}

    # 4. Run an evaluation
    evaluate(
        chatbot,
        data=dataset,
        evaluators=[is_concise],
        experiment_prefix="my first experiment "
    )

`,
    typescript`import { Client } from "langsmith";
import { evaluate } from "langsmith/evaluation";
import type { EvaluationResult } from "langsmith/evaluation";
import type { Run, Example } from "langsmith/schemas";\n
// 1. Define a dataset
const client = new Client();
const datasetName = "my first dataset"
const dataset = await client.clonePublicDataset(
"https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d",
{ datasetName: datasetName }
)\n
// 2. Define an evaluator
function isConcise(rootRun: Run, example: Example): EvaluationResult {
const score = rootRun.outputs?.outputs.length < 3 \* example.outputs?.answer.length;
return { key: "is_concise", score: score };
}\n
// 3. Run an evaluation
// For more info on evaluators, see: https://docs.smith.langchain.com/concepts/evaluation#evaluators
await evaluate(
(exampleInput) => {
return {
answer: exampleInput.question + " Good question. I don't know the answer"
};
}, {
data: datasetName,
evaluators: [isConcise],
experimentPrefix: "my first experiment ",
});`,
]}
groupId="client-language"
/>

## 5. View Experiments UI

Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of your evaluation.

![](./how_to_guides/evaluation/static/view_experiment.gif)

## Next steps

For conceptual explanations see the [Conceptual guide](./evaluation/concepts).
See the [How-to guides](./evaluation/how_to_guides) for answers to “How do I….?” format questions.
For end-to-end walkthroughs see [Tutorials](./evaluation/tutorials).
For comprehensive descriptions of every class and function see the [API reference](https://langsmith-sdk.readthedocs.io/en/latest/evaluation.html).
