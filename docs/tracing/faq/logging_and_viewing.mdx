---
sidebar_label: How to log and view traces
sidebar_position: 1
---

import {
CodeTabs,
PythonBlock,
TypeScriptBlock,
LangChainPyBlock,
LangChainJSBlock,
APIBlock,
} from "@site/src/components/InstructionsWithCode";

import { AccessRunIdBlock } from "@site/src/components/TracingFaq";

# How to log and view traces to LangSmith

LangSmith makes it easy to log and view traces from your LLM application, regardless of which language or framework you use.

### Logging Traces

There are multiple ways to logs traces to LangSmith using the LangSmith SDK or API, OpenAI's Python client, or LangChain.

When using the Python SDK, take special note of the `traceable` decorator and `wrap_openai`, as these methods can be easier to use than the `RunTree` API.

:::note
Please follow the [Setup](/setup) guide to learn how to sign up and create an API key.
:::
:::note
Please make sure to set the `LANGCHAIN_API_KEY` environment variable to your API key before running the examples below.
Additionally, you will need to set `LANGCHAIN_TRACING_V2='true'` if you plan to use either
* LangChain (Python or JS)
* `@traceable` decorator or `wrap_openai` method in the Python SDK
:::

<CodeTabs
    tabs={[
        PythonBlock(`# To run the example below, ensure the environment variable OPENAI_API_KEY is set
from typing import Any, Iterable
import openai
from langsmith import traceable
from langsmith.run_trees import RunTree
from langsmith.wrappers import wrap_openai\n
### OPTION 1: Use RunTree API (more explicit) ###
# This can be a user input to your app
question = "Can you summarize this morning's meetings?"\n
# Create a top-level run
pipeline = RunTree(
    name="Chat Pipeline Run Tree",
    run_type="chain",
    inputs={"question": question}
)\n
# This can be retrieved in a retrieval step
context = "During this morning's meeting, we solved all world conflict."\n
messages = [
    { "role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context." },
    { "role": "user", "content": f"Question: {question}\nContext: {context}"}
]\n
# Create a child run
child_llm_run = pipeline.create_child(
    name="OpenAI Call",
    run_type="llm",
    inputs={"messages": messages},
)\n
# Generate a completion
client = openai.Client()
chat_completion = client.chat.completions.create(
    model="gpt-3.5-turbo", messages=messages
)\n
# End the runs and log them
child_llm_run.end(outputs=chat_completion)
child_llm_run.post()\n
pipeline.end(outputs={"answer": chat_completion.choices[0].message.content})
pipeline.post()\n
### OPTION 2: Use traceable decorator and OpenAI Client ###
# Optional: wrap the openai client to add tracing directly
# This can also be done with a traceable decorator
client = wrap_openai(openai.Client())\n
@traceable(run_type="tool", name="Retrieve Context")
def my_tool(question: str) -> str:
    return "During this morning's meeting, we solved all world conflict."\n
@traceable(name="Chat Pipeline Traceable")
def chat_pipeline(question: str):
    context = my_tool(question)
    messages = [
        { "role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context." },
        { "role": "user", "content": f"Question: {question}\nContext: {context}"}
    ]
    chat_completion = client.chat.completions.create(
        model="gpt-3.5-turbo", messages=messages
    )
    return chat_completion.choices[0].message.content\n
chat_pipeline("Can you summarize this morning's meetings?")`),
        TypeScriptBlock(`// To run the example below, ensure the environment variable OPENAI_API_KEY is set
import OpenAI from "openai";
import { RunTree } from "langsmith";\n
// This can be a user input to your app
const question = "Can you summarize this morning's meetings?";\n
const pipeline = new RunTree({
    name: "Chat Pipeline",
    run_type: "chain",
    inputs: { question }
});\n
// This can be retrieved in a retrieval step
const context = "During this morning's meeting, we solved all world conflict.";\n
const messages = [
    { role: "system", content: "You are a helpful assistant. Please respond to the user's request only based on the given context." },
    { role: "user", content: \`Question: \${question}\nContext: \${context}\` }
];\n
// Create a child run
const childRun = await pipeline.createChild({
    name: "OpenAI Call",
    run_type: "llm",
    inputs: { messages },
});\n
// Generate a completion
const client = new OpenAI();
const chatCompletion = await client.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: messages,
});\n
// End the runs and log them
childRun.end(chatCompletion);
await childRun.postRun();\n
pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });
await pipeline.postRun();`),
        LangChainPyBlock(`# No extra code is needed to log a trace to LangSmith when using LangChain Python.
# Just run your LangChain code as you normally would with the LANGCHAIN_TRACING_V2 environment variable set to 'true' and the LANGCHAIN_API_KEY environment variable set to your API key.
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser\n
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Please respond to the user's request only based on the given context."),
    ("user", "Question: {question}\nContext: {context}")
])
model = ChatOpenAI(model="gpt-3.5-turbo")
output_parser = StrOutputParser()\n
chain = prompt | model | output_parser\n
question = "Can you summarize this morning's meetings?"
context = "During this morning's meeting, we solved all world conflict."
chain.invoke({"question": question, "context": context})`),
LangChainJSBlock(`// No extra code is needed to log a trace to LangSmith when using LangChain JS.
// Just run your LangChain code as you normally would with the LANGCHAIN_TRACING_V2 environment variable set to 'true' and the LANGCHAIN_API_KEY environment variable set to your API key.
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";\n
const prompt = ChatPromptTemplate.fromMessages([
["system", "You are a helpful assistant. Please respond to the user's request only based on the given context."],
["user", "Question: {question}\\nContext: {context}"],
]);
const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });
const outputParser = new StringOutputParser();\n
const chain = prompt.pipe(model).pipe(outputParser);\n
const question = "Can you summarize this morning's meetings?"
const context = "During this morning's meeting, we solved all world conflict."
await chain.invoke({ question: question, context: context });`),
APIBlock(`# To run the example below, ensure the environment variable OPENAI_API_KEY is set
# Here, we'll show you to use the requests library in Python to log a trace, but you can use any HTTP client in any language.
import openai
import requests
from datetime import datetime
from uuid import uuid4\n
def post_run(run_id, name, run_type, inputs, parent_id=None):
    """Function to post a new run to the API."""
    data = {
        "id": run_id.hex,
        "name": name,
        "run_type": run_type,
        "inputs": inputs,
        "start_time": datetime.utcnow().isoformat(),
    }
    if parent_id:
        data["parent_run_id"] = parent_id.hex
    requests.post(
        "https://api.smith.langchain.com/runs",
        json=data,
        headers=headers
    )\n
def patch_run(run_id, outputs):
    """Function to patch a run with outputs."""
    requests.patch(
        f"https://api.smith.langchain.com/runs/{run_id}",
        json={
            "outputs": outputs,
            "end_time": datetime.utcnow().isoformat(),
        },
        headers=headers,
    )\n
# Send your API Key in the request headers
headers = {"x-api-key": "<YOUR API KEY>"}\n
# This can be a user input to your app
question = "Can you summarize this morning's meetings?"\n
# This can be retrieved in a retrieval step
context = "During this morning's meeting, we solved all world conflict."
messages = [
    {"role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context."},
    {"role": "user", "content": f"Question: {question}\nContext: {context}"}
]\n
# Create parent run
parent_run_id = uuid4()
post_run(parent_run_id, "Chat Pipeline", "chain", {"question": question})\n
# Create child run
child_run_id = uuid4()
post_run(child_run_id, "OpenAI Call", "llm", {"messages": messages}, parent_run_id)\n
# Generate a completion
client = openai.Client()
chat_completion = client.chat.completions.create(model="gpt-3.5-turbo", messages=messages)\n
# End runs
patch_run(child_run_id, chat_completion.dict())
patch_run(parent_run_id, {"answer": chat_completion.choices[0].message.content})`)]}
    groupId="client-language"
/>

### Viewing Traces [TODO]

### Setting a sampling rate for tracing

To downsample the number of traces logged to LangSmith, set the `LANGCHAIN_TRACING_SAMPLING_RATE` environment variable to
any float between 0 (no traces) and 1 (all traces). This requires a python SDK version >= 0.0.84, and a JS SDK version >= 0.0.64.
    For instance, setting the following environment variable will filter out 25% of traces:

```bash
export LANGCHAIN_TRACING_SAMPLING_RATE=0.75
```

This works for the `traceable` decorator and `RunTree` objects.

### Turning off tracing

If you've decided you no longer want to trace your runs, you can remove the environment variables configured to start tracing in the first place.
By unsetting the `LANGCHAIN_TRACING_V2` environment variable, traces will no longer be logged to LangSmith.
Note that this currently does not affect the `RunTree` objects.

This setting works both with LangChain and the LangSmith SDK, in both Python and TypeScript.

### How do I get the URL of the run?

Runs are streamed to whichever project you have configured ("default" if none is set), and you can view them by opening the project page. To programmatically access the run's URL, you can
use the LangSmith client. This can be conveniently used in conjunction with [the above method to get the run ID from a call](#how-do-i-get-the-run-id-from-a-call). Below is an example.

<CodeTabs
    tabs={[
        PythonBlock(`from langsmith import Client\n
client = Client()
run = client.read_run("<run_id>")
print(run.url)`),
        TypeScriptBlock(`import { Client } from "langsmith";\n
const client = new Client();
const runUrl = await client.getRunUrl({runId: "<run_id>"});
console.log(runUrl);
`),
    ]}
    groupId="client-language"
/>

### How do I delete traces in a project?

You can delete a project, along with all its associated traces and other information, in the UI or by using the LangSmith client.

Below is an example using the SDK:

<CodeTabs
    tabs={[
        PythonBlock(`from langsmith import Client\n
client = Client()
client.delete_project(project_name="<project_name>")`),
        TypeScriptBlock(`import { Client } from "langsmith";\n
const client = new Client();
await client.deleteProject({projectName: "<project_name>"});
`),
    ]}
    groupId="client-language"
/>
