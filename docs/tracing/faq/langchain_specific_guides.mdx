---
sidebar_label: LangChain-Specific Guides
sidebar_position: 6 
---

import {
    CodeTabs,
    PythonBlock,
    TypeScriptBlock,
    LangChainPyBlock,
    LangChainJSBlock,
    APIBlock,
} from "@site/src/components/InstructionsWithCode";

import {
    AccessRunIdBlock
} from "@site/src/components/TracingFaq";


# LangChain-specific guides

This sections contains guides that are specific to users using LangChain (both Python and JavaScript).

### Grouping runs from multi-turn interactions

When using LangChain, you may want to group runs from multi-turn interactions together.

With chatbots, copilots, and other common LLM design patterns, users frequently interact with your model over multiple interactions. Each invocation of your model is logged as a separate trace, but you can group these traces together using metadata (see [how to add metadata to a run](#how-do-i-add-metadata-or-tags-to-a-run) above for more information).
Below is a minimal example with LangChain, but **the same idea applies when using the LangSmith SDK or API.**

<CodeTabs
    tabs={[
        LangChainPyBlock(`from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI\n
chain = (
    ChatPromptTemplate.from_messages(
        [
            ("system", "You are a helpful AI."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{message}"),
        ]
    )
    | ChatOpenAI(model="gpt-3.5-turbo")
    | StrOutputParser()
)\n
conversation_id = "101e8e66-9c68-4858-a1b4-3b0e3c51a933"\n
chat_history = []
message = HumanMessage(content="Hi there")
response = ""
for chunk in chain.stream(
    {
        "message": message,
        "chat_history": chat_history,
    },
    # highlight-next-line
    config={"metadata": {"conversation_id": conversation_id}},
):
    print(chunk, end="")
    response += chunk
print()
chat_history.extend(
    [
        message,
        AIMessage(content=response),
    ]
)\n
# ... Next message comes in
next_message = HumanMessage(content="I don't need much assistance, actually.")
for chunk in chain.stream(
    {
        "message": next_message,
        "chat_history": chat_history,
    },
    # highlight-next-line
    config={"metadata": {"conversation_id": conversation_id}},
):
    print(chunk, end="")
    response += chunk
`),
        LangChainJSBlock(`import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";\n
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful AI."],
  ["user", "{message}"],
]);
const chain = prompt
  .pipe(new ChatOpenAI({ model: "gpt-3.5-turbo" }))
  .pipe(new StringOutputParser());\n
const conversationId = "101e8e66-9c68-4858-a1b4-3b0e3c51a933";
const chatHistory = [];
const message = { content: "Hi there" };
let response = "";
for await (const chunk of await chain.stream(
  {
    message,
  },
  {
    // highlight-next-line
    metadata: { conversation_id: conversationId },
  }
)) {
  process.stdout.write(chunk);
  response += chunk;
}
console.log();
chatHistory.push(message, { content: response });\n
// ... Next message comes in
const nextMessage = { content: "I don't need much assistance, actually." };
for await (const chunk of await chain.stream(
  {
    message: nextMessage,
  },
  {
    // highlight-next-line
    metadata: { conversation_id: conversationId },
  }
)) {
  process.stdout.write(chunk);
  response += chunk;
}`),
    ]}
    groupId="client-language"
/>

To view all the traces from that conversation in LangSmith, you can query the project using a metadata filter:

```bash
has(metadata, '{"conversation_id":"101e8e66-9c68-4858-a1b4-3b0e3c51a933"}')
```

This will return all traces with the specified conversation ID.

### Getting a run ID from a LangChain call

In Typescript, the run ID is returned in the call response under the `__run` key. In python, we recommend using the run collector callback.
Below is an example:

<AccessRunIdBlock />

If using the API or SDK, you can send the run ID as a parameter to `RunTree` or the `traceable` decorator.

### Tracing without environment variables

Some situations don't permit the use of environment variables or don't expose `process.env`. This is mostly pertinent when running LangChain apps in certain JavaScript runtime environments. To add tracing in these situations, you can manually create the `LangChainTracer` callback and pass it to the chain, LLM, or other LangChain component, either when initializing or in the call itself. This is the same tactic used for [changing the tracer project](#how-do-i-change-the-tracer-project) within a program.

Example:

<CodeTabs
    tabs={[
        LangChainPyBlock(`from langchain.callbacks import LangChainTracer
from langchain_openai import ChatOpenAI
from langsmith import Client\n
callbacks = [
  LangChainTracer(
    project_name="YOUR_PROJECT_NAME_HERE",
    client=Client(
      api_url="https://api.smith.langchain.com",
      api_key="YOUR_API_KEY_HERE"
    )
  )
]\n
llm = ChatOpenAI()
llm.invoke("Hello, world!", config={"callbacks": callbacks})
`),
        LangChainJSBlock(`import { Client } from "langsmith";
import { LangChainTracer } from "langchain/callbacks";
import { ChatOpenAI } from "langchain/chat_models/openai";\n
const callbacks = [
  new LangChainTracer({
    projectName: "YOUR_PROJECT_NAME_HERE",
    client: new Client({
      apiUrl: "https://api.smith.langchain.com",
      apiKey: "YOUR_API_KEY_HERE",
    }),
  }),
];
const llm = new ChatOpenAI({});
await llm.invoke("Hello, world!", { callbacks });
`),
    ]}
    groupId="client-language"
/>

This tactic is also useful for when you have multiple chains running in a shared environment but want to log their run traces to different projects.

### Ensuring all traces are submitted before exiting

In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that if you're process may end before all traces are successfully posted to LangSmith. This is especially prevelant in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.

In LangChain JS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"true"`.

For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example:

<CodeTabs
    tabs={[
        LangChainPyBlock(`from langchain_openai import ChatOpenAI
from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n
llm = ChatOpenAI()
try:
    llm.invoke("Hello, World!")
finally:
    # highlight-next-line
    wait_for_all_tracers()
`),
        LangChainJSBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { awaitAllCallbacks } from "langchain/callbacks";\n
try {
  const llm = new ChatOpenAI();
  const response = await llm.invoke("Hello, World!");
} catch (e) {
  // handle error
} finally {
  await awaitAllCallbacks();
}
`),
    ]}
    groupId="client-language"
/>
