---
sidebar_position: 2
---

# Run the playground against an OpenAI-compliant model provider/proxy

The LangSmith playground allows you to use any model that is compliant with the OpenAI API.

## Deploy an OpenAI-compliant model

Many providers offer OpenAI-compliant models or proxy services that wrap existing models with an OpenAI-compatible API. Some popular options include:

- [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)
- [Ollama](https://ollama.com/)

These tools allow you to deploy models with an API endpoint that follows the OpenAI specification. For implementation details, refer to the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat).

## Use the model in the LangSmith Playground

Once you have deployed a model server, you can use it in the LangSmith Playground.

### Configure the playground

1. Open the LangSmith Playground
2. Change the provider to `Custom Model Endpoint`
3. Enter your model's endpoint URL in the `Base URL` field
4. Configure any additional configuration parameters

![Custom Model Endpoint](./static/custom_model_endpoint.png)

The playground uses [`ChatOpenAI`](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) from `langchain-openai` under the hood, automatically configuring it with your custom endpoint as the `base_url`.

### Testing the connection

Click `Start` to test the connection. If properly configured, you should see your model's responses appear in the playground. You can then experiment with different prompts and parameters.

## Save your model configuration

To reuse your custom model configuration in future sessions, learn how to save and manage your settings [here](./managing_model_configurations).
