# Configuring LangSmith for Scale

A self-hosted LangSmith instance can handle a large number of traces. The default configuration for the deployment can handle substantial load, and you can configure your deployment to be able to achieve higher scale.

In the table below, we provide some high level guidance for LangSmith configurations. Further below, we go into detail on the recommended configurations. Note that we measure scale here in terms of traces per second (TPS). We strongly recommend running high load against a kubernetes deployment of LangSmith.

| Desired traces per second (TPS) | Queue worker replicas | Frontend replicas | Platform backend replicas | Clickhouse config               | Redis cache size |
| ------------------------------- | --------------------- | ----------------- | ------------------------- | ------------------------------- | ---------------- |
| **10 TPS**                      | 3 (default)           | 1 (default)       | 3 (default)               | 4 vCPUs, 16 GB Memory (default) | 2 GB (default)   |
| **100 TPS**                     | **10**                | 1 (default)       | 3 (default)               | 4 vCPUs, 16 GB Memory (default) | **14+ GB**       |
| **1000 TPS**                    | **160**               | **4**             | **20**                    | **9 vCPUs, 32 GB Memory**       | **200+ GB**      |

:::note
Your exact usage pattern of the product may require more tuning of resources. If that is the case, please reach out to the LangSmith team for any questions.
:::
:::note
The configuration recommendations also assume that you are using a relatively new version of the SDK and LangSmith. If you haven't updated in a while, please do so. We are constantly pushing performance improvements, and will keep this page updated accordingly.
:::

## 10 Traces Per Second (TPS)

The default LangSmith configuration mentioned in our [prerequisites section](../installation/kubernetes#prerequisites) should be able to handle 10 TPS.

## 100 TPS

To achieve 100 TPS, we recommend the following updates to your configuration:

- A Redis cache of at least 14 GB. We also recommend a 1 hour TTL on the redis cache.
- 10 queue worker replicas.
- Increasing the clickhouse PVC to store the traces.

Here is an example `values.yaml` snippet for this configuration:

```yaml
config:
  settings:
    redisRunsExpirySeconds: "3600"

queue:
  deployment:
    replicas: 10

redis:
  statefulSet:
    resources:
      requests:
        memory: 14Gi
      limits:
        memory: 14Gi

  # -- For external redis instead use something like below --
  # external:
  #   enabled: true
  #   connectionUrl: "<URL>" OR existingSecretName: "<SECRET-NAME>"

clickhouse:
  statefulSet:
    persistence:
      # This may depend on your configured TTL.
      # We recommend 60Gi for every shortlived TTL day if operating at this scale constantly.
      size: 420Gi # This assumes 7 days TTL and operating a this scale constantly.
```

:::note
Note that we recommend having an external Redis cache. If this is the case, you will need to ensure your Redis cache is configured with at least 14 GB instead of the resource configuration in the values file shown above.
:::

## 1000 TPS

In order to achieve 1000 TPS on a self-hosted kubernetes LangSmith deployment, we recommend the following updates to the LangSmith configuration:

- An external Redis cache of at least 200 GB. We also recommend a 1 hour TTL on the redis cache.
- 8 vCPU and 24 GB of memory on your clickhouse instance.
- 160 queue workers with 20 jobs per worker
- 20 platform-backend pods
- 4 frontend pods. We use these as a reverse proxy for inbound requests.

Here is a `values.yaml` snippet configuring the recommendations above:

```yaml
frontend:
  deployment:
    replicas: 4 # OR enable autoscaling to this level (example below)
# autoscaling:
#   enabled: true
#   maxReplicas: 4
#   minReplicas: 2

platformBackend:
  deployment:
    replicas: 20 # OR enable autoscaling to this level (example below)
    resources:
      requests:
        cpu: "1600m"
# autoscaling:
#   enabled: true
#   maxReplicas: 20
#   minReplicas: 8

## Note that we are actively working on improving performance on this service.
queue:
  deployment:
    replicas: 160 # OR enable autoscaling to this level (example below)
# autoscaling:
#   enabled: true
#   maxReplicas: 160
#   minReplicas: 40

## Ensure your Redis cache is at least 200 GB
redis:
  external:
    enabled: true
    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)

clickhouse:
  statefulSet:
    persistence:
      # This may depend on your configured TTL (see config section).
      # We recommend 600Gi for every shortlived TTL day if operating at this scale constantly.
      size: 4200Gi # This assumes 7 days TTL and operating a this scale constantly.
    resources:
      requests:
        cpu: "9"
        memory: "32Gi"
      limits:
        cpu: "12"
        memory: "48Gi"

config:
  blobStorage:
    ## Please also set the other keys to connect to your blob storage. See configuration section.
    enabled: true
  settings:
    redisRunsExpirySeconds: "3600"
# ttl:
#   enabled: true
#   ttl_period_seconds:
#     longlived: "7776000"  # 90 days
#     shortlived: "604800"  # 7 days

# These are important environment variables to set.
commonEnv:
  - name: "CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT"
    value: "0"
```

:::important
Please also make sure that the kubernetes cluster is configured with sufficent resources to scale to the recommended size. After deployment, all of the pods in the kubernetes cluster should be in a `Running` state. Pods stuck in `Pending` may indicate that you are reaching node pool limits or need larger nodes.
:::

:::important
Ensure any ingress controller deployed on the cluster is able to handle the desired load. In our internal testing, we saw bottlenecks in our Nginx controller and make the following updates to achieve this scale:

```bash
    replicaCount = 4
    config = {
        proxy-request-buffering    = "off"
        client-body-buffer-size    = "32m"
        proxy-body-size            = "100m"
    }
```

:::
