---
sidebar_position: 3
---

import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
  LangChainPyBlock,
  LangChainJSBlock,
} from "@site/src/components/InstructionsWithCode";

# Log traces to specific project

You can change the destination project of your traces both statically through environment variables and dynamically at runtime.

## Set the destination project statically

As mentioned in the [Tracing Concepts](/observability/concepts#projects) section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the project is set to `default`. You can set the `LANGCHAIN_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.

```bash
export LANGCHAIN_PROJECT=my-custom-project
```

If the project specified does not exist, it will be created automatically when the first trace is ingested.

## Set the destination project dynamically

You can also set the project name at program runtime in various ways, depending on how you are [annotating your code for tracing](./annotate_code). This is useful when you want to log traces to different projects within the same application.

:::note
Setting the project name dynamically using one of the below methods overrides the project name set by the `LANGCHAIN_PROJECT` environment variable.
:::

<CodeTabs
  tabs={[
    PythonBlock(`import openai
from langsmith import traceable
from langsmith.run_trees import RunTree\n
client = openai.Client()\n
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]\n
# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith
# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work
@traceable(
    run_type="llm",
    name="OpenAI Call Decorator",
    # highlight-next-line
    project_name="My Project"
)
def call_openai(
    messages: list[dict], model: str = "gpt-3.5-turbo"
) -> str:
    return client.chat.completions.create(
        model=model,
        messages=messages,
    ).choices[0].message.content\n
# Call the decorated function
call_openai(messages)\n
# You can also specify the Project via the project_name parameter
# This will override the project_name specified in the @traceable decorator
call_openai(
    messages,
    # highlight-next-line
    langsmith_extra={"project_name": "My Overridden Project"},
)\n
# The wrapped OpenAI client accepts all the same langsmith_extra parameters
# as @traceable decorated functions, and logs traces to LangSmith automatically.
# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.
from langsmith import wrappers
wrapped_client = wrappers.wrap_openai(client)
wrapped_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages,
    # highlight-next-line
    langsmith_extra={"project_name": "My Project"},
)\n\n
# Alternatively, create a RunTree object
# You can set the project name using the project_name parameter
rt = RunTree(
    run_type="llm",
    name="OpenAI Call RunTree",
    inputs={"messages": messages},
    # highlight-next-line
    project_name="My Project"
)
chat_completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages,
)
# End and submit the run
rt.end(outputs=chat_completion)
rt.postRun()
`),
    TypeScriptBlock(`import OpenAI from "openai";
import { traceable } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";
import { RunTree} from "langsmith";\n
const client = new OpenAI();\n
const messages = [
    {role: "system", content: "You are a helpful assistant."},
    {role: "user", content: "Hello!"}
];\n
const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {
    const completion = await client.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: messages,
    });
    return completion.choices[0].message.content;
},{
    run_type: "llm",
    name: "OpenAI Call Traceable",
    // highlight-next-line
    project_name: "My Project"
});
// Call the traceable function
await traceableCallOpenAI(messages, "gpt-3.5-turbo");\n
// Create and use a RunTree object
const rt = new RunTree({
    runType: "llm",
    name: "OpenAI Call RunTree",
    inputs: { messages },
    // highlight-next-line
    project_name: "My Project"
});
await rt.postRun();\n
// Execute a chat completion and handle it within RunTree
rt.end({outputs: chatCompletion});
await rt.patchRun();`),
  ]}
  groupId="client-language"
/>
