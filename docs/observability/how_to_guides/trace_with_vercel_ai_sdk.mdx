---
sidebar_position: 17
---

import {
  CodeTabs,
  ShellBlock,
  typescript,
} from "@site/src/components/InstructionsWithCode";

# Trace with the Vercel AI SDK (JS/TS only)

You can use LangSmith to trace runs from the [Vercel AI SDK](https://sdk.vercel.ai/docs/introduction) using OpenTelemetry (OTEL). This guide will walk through an example.

:::note
OpenTelemetry support is only available in `langsmith` JS SDK version `>=0.3.35`. If you are on an older version of `langsmith`, see [this page](./trace_with_vercel_ai_sdk_legacy).

OpenTelemetry support is currently experimental.
:::

## 0. Installation

Install the Vercel AI SDK and required OTEL packages. We use their OpenAI integration for the code snippets below, but you can use any of their
other options as well.

<CodeTabs
  groupId="client-language"
  tabs={[
    {
      value: "npm",
      label: "npm",
      language: "bash",
      content: `npm install ai @ai-sdk/openai zod @opentelemetry/sdk-trace-base @opentelemetry/exporter-trace-otlp-proto @opentelemetry/context-async-hooks`,
    },
    {
      value: "typescript",
      label: "yarn",
      language: "bash",
      content: `yarn add ai @ai-sdk/openai zod @opentelemetry/sdk-trace-base @opentelemetry/exporter-trace-otlp-proto @opentelemetry/context-async-hooks`,
    },
    {
      value: "pnpm",
      label: "pnpm",
      language: "bash",
      content: `pnpm add ai @ai-sdk/openai zod @opentelemetry/sdk-trace-base @opentelemetry/exporter-trace-otlp-proto @opentelemetry/context-async-hooks`,
    },
  ]}
/>

## 1. Configure your environment

<CodeTabs
  tabs={[
    ShellBlock(`export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>
export OTEL_ENABLED=true
# This example uses OpenAI, but you can use any LLM provider of choice
export OPENAI_API_KEY=<your-openai-api-key>`),
  ]}
  groupId="client-language"
/>

## 2. Log a trace

### Node.js

To start tracing, you will need to import and call the `initializeOTEL` method at the start of your code:

```ts
import { initializeOTEL } from "langsmith/experimental/otel/setup";

const { DEFAULT_LANGSMITH_SPAN_PROCESSOR } = initializeOTEL();
```

Afterwards, add the `experimental_telemetry` argument to your AI SDK calls that you want to trace.

:::info
Do not forget to call `await DEFAULT_LANGSMITH_SPAN_PROCESSOR.shutdown();` before your application shuts down in order to flush any remaining traces to LangSmith.
:::

```ts
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

let result;

try {
  result = await generateText({
    model: openai("gpt-4.1-nano"),
    prompt: "Write a vegetarian lasagna recipe for 4 people.",
    // highlight-start
    experimental_telemetry: {
      isEnabled: true,
    },
    // highlight-end
  });
} finally {
  // highlight-next-line
  await DEFAULT_LANGSMITH_SPAN_PROCESSOR.shutdown();
}
```

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/21d33490-d522-4928-a944-a09e988d539c/r).

You can also trace runs with tool calls:

```ts
import { generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

await generateText({
  model: openai("gpt-4.1-nano"),
  messages: [
    {
      role: "user",
      content: "What are my orders and where are they? My user ID is 123",
    },
  ],
  tools: {
    listOrders: tool({
      description: "list all orders",
      parameters: z.object({ userId: z.string() }),
      execute: async ({ userId }) =>
        `User ${userId} has the following orders: 1`,
    }),
    viewTrackingInformation: tool({
      description: "view tracking information for a specific order",
      parameters: z.object({ orderId: z.string() }),
      execute: async ({ orderId }) =>
        `Here is the tracking information for ${orderId}`,
    }),
  },
  experimental_telemetry: {
    isEnabled: true,
  },
  maxSteps: 10,
});
```

Which results in a trace like [this one](https://smith.langchain.com/public/e6122734-2762-4ae0-986b-0cbe4d68692f/r).

### With `traceable`

You can wrap `traceable` calls around or within AI SDK tool calls. If you do so, we recommend you initialize a LangSmith `client` instance
that you pass into each `traceable`, then call `client.awaitPendingTraceBatches();` to ensure all traces flush. If you do this, you do not need
to manually call `shutdown()` or `forceFlush()` on the `DEFAULT_LANGSMITH_SPAN_PROCESSOR`. Here's an example:

```ts
// highlight-start
import { initializeOTEL } from "langsmith/experimental/otel/setup";

initializeOTEL();
// highlight-end

import { Client } from "langsmith";
import { traceable } from "langsmith/traceable";
import { generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

const client = new Client();

const wrappedText = traceable(
  async (content: string) => {
    const { text } = await generateText({
      model: openai("gpt-4.1-nano"),
      messages: [{ role: "user", content }],
      tools: {
        listOrders: tool({
          description: "list all orders",
          parameters: z.object({ userId: z.string() }),
          execute: async ({ userId }) => {
            const getOrderNumber = traceable(
              async () => {
                return "1234";
              },
              { name: "getOrderNumber" }
            );
            const orderNumber = await getOrderNumber();
            return `User ${userId} has the following order: ${orderNumber}`;
          },
        }),
      },
      experimental_telemetry: {
        isEnabled: true,
      },
      maxSteps: 10,
    });

    return { text };
  },
  // highlight-next-line
  { name: "parentTraceable", client }
);

let result;

try {
  result = await wrappedText("What are my orders?");
} finally {
  // highlight-next-line
  await client.awaitPendingTraceBatches();
}
```

The resulting trace will look [like this](https://smith.langchain.com/public/296a0134-f3d4-4e54-afc7-b18f2c190911/r).

### Next.js

The [default OTEL setup](https://nextjs.org/docs/app/api-reference/file-conventions/instrumentation) for Next.js will trace all routes, including
those that do not contain LLM traces. We instead suggest manually instrumenting specific routes by creating and passing a tracer as shown below:

```ts
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";
import { traceable } from "langsmith/traceable";

import { initializeOTEL } from "langsmith/experimental/otel/setup";
import { LangSmithOTLPTraceExporter } from "langsmith/experimental/otel/exporter";
import { BatchSpanProcessor, BasicTracerProvider } from "@opentelemetry/sdk-trace-base";
import { AsyncHooksContextManager } from "@opentelemetry/context-async-hooks";
import { context } from "@opentelemetry/api";

import { after } from "next/server";

const exporter = new LangSmithOTLPTraceExporter();
const processor = new BatchSpanProcessor(exporter);
const contextManager = new AsyncHooksContextManager();

contextManager.enable();
context.setGlobalContextManager(contextManager);

const provider = new BasicTracerProvider({
  spanProcessors: [processor],
});

// Pass instantiated provider and context manager to LangSmith
initializeOTEL({
  globalTracerProvider: provider,
  globalContextManager: contextManager,
});

// highlight-next-line
const tracer = provider.getTracer("ai-sdk-telemetry");

export async function GET() {
  after(async () => {
    await provider.shutdown();
  });

  const wrappedText = traceable(
    async (content: string) => {
      const { text } = await generateText({
        model: openai("gpt-4.1-nano"),
        messages: [{ role: "user", content }],
        experimental_telemetry: {
          isEnabled: true,
          // highlight-next-line
          tracer,
        },
        maxSteps: 10,
      });
  
      return { text };
    },
    // highlight-next-line
    { name: "parentTraceable", tracer }
  );
  
  const { text } = await wrappedText("Why is the sky blue?");
  
  return new Response(text);
}
```

### Sentry

If you're using Sentry, you can attach the LangSmith trace exporter to Sentry's default OpenTelemetry instrumentation as show in the example below.

:::caution

At time of writing, Sentry only supports OTEL v1 packages. LangSmith supports both v1 and v2, but you MUST make sure you install
OTEL v1 packages in order to make instrumentation work.

<CodeTabs
  groupId="client-language"
  tabs={[
    {
      value: "npm",
      label: "npm",
      language: "bash",
      content: `npm install @opentelemetry/sdk-trace-base@1.30.1 @opentelemetry/exporter-trace-otlp-proto@0.57.2 @opentelemetry/context-async-hooks@1.30.1`,
    },
    {
      value: "typescript",
      label: "yarn",
      language: "bash",
      content: `yarn add @opentelemetry/sdk-trace-base@1.30.1 @opentelemetry/exporter-trace-otlp-proto@0.57.2 @opentelemetry/context-async-hooks@1.30.1`,
    },
    {
      value: "pnpm",
      label: "pnpm",
      language: "bash",
      content: `pnpm add @opentelemetry/sdk-trace-base@1.30.1 @opentelemetry/exporter-trace-otlp-proto@0.57.2 @opentelemetry/context-async-hooks@1.30.1`,
    },
  ]}
/>

:::

```ts
import { initializeOTEL } from "langsmith/experimental/otel/setup";

import { LangSmithOTLPTraceExporter } from "langsmith/experimental/otel/exporter";
import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";

import { traceable } from "langsmith/traceable";
import { generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

import * as Sentry from "@sentry/node";
import { Client } from "langsmith";

const exporter = new LangSmithOTLPTraceExporter();

const spanProcessor = new BatchSpanProcessor(exporter);

const sentry = Sentry.init({
  dsn: "...",
  tracesSampleRate: 1.0,
  openTelemetrySpanProcessors: [spanProcessor],
});

initializeOTEL({
  globalTracerProvider: sentry?.traceProvider,
});

const wrappedText = traceable(
  async (content: string) => {
    const { text } = await generateText({
      model: openai("gpt-4.1-nano"),
      messages: [{ role: "user", content }],
      experimental_telemetry: {
        isEnabled: true,
      },
      maxSteps: 10,
    });

    return { text };
  },
  { name: "parentTraceable" }
);

let result;

try {
  result = await wrappedText("What color is the sky?");
} finally {
  await sentry?.traceProvider?.shutdown();
}
```
