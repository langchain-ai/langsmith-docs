---
sidebar_label: Quick start
sidebar_position: 1
table_of_contents: true
---

import Tabs from "@theme/Tabs";
import CodeBlock from "@theme/CodeBlock";
import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
  typescript,
} from "@site/src/components/InstructionsWithCode";
import {
  LangChainInstallationCodeTabs,
  LangChainQuickStartCodeTabs,
  ConfigureEnvironmentCodeTabs,
  RunTreeQuickStartCodeTabs,
  ConfigureSDKEnvironmentCodeTabs,
  PythonSDKTracingCode,
  TypeScriptSDKTracingCode,
} from "@site/src/components/QuickStart";
import { ClientInstallationCodeTabs } from "@site/src/components/ClientInstallation";
import DocCardList from "@theme/DocCardList";
import { RegionalUrl } from "@site/src/components/RegionalUrls";

# Get started with LangSmith

**LangSmith** is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!

## 1. Install LangSmith

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "bash",
      content: `pip install -U langsmith`,
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "bash",
      content: `yarn add langsmith`,
    },
  ]}
  groupId="client-language"
/>

## 2. Create an API key

To create an API key head to the <RegionalUrl text='Settings page' suffix='/settings' />. Then click **Create API Key.**

## 3. Set up your environment

<ConfigureSDKEnvironmentCodeTabs />

## 4. Log your first trace

:::tip Tracing to LangSmith for LangChain users
There is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).

We've outlined a tracing guide specifically for LangChain users [here](./how_to_guides/tracing/trace_with_langchain).
:::

We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use `traceable`. See more on the [Annotate code for tracing](./how_to_guides/tracing/annotate_code) page.

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "python",
      content: PythonSDKTracingCode(),
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "typescript",
      content: TypeScriptSDKTracingCode(),
    },
  ]}
  groupId="client-language"
/>

- View a [sample output trace](https://smith.langchain.com/public/b37ca9b1-60cd-4a2a-817e-3c4e4443fdc0/r).
- Learn more about tracing in the [how-to guides](./how_to_guides/index.md).

## 5. Run your first evaluation

Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "python",
      content: `from langsmith import Client
from langsmith.evaluation import evaluate\n
client = Client()\n
# Define dataset: these are your test cases
dataset_name = "Sample Dataset"
dataset = client.create_dataset(dataset_name, description="A sample dataset in LangSmith.")
client.create_examples(
    inputs=[
        {"postfix": "to LangSmith"},
        {"postfix": "to Evaluations in LangSmith"},
    ],
    outputs=[
        {"output": "Welcome to LangSmith"},
        {"output": "Welcome to Evaluations in LangSmith"},
    ],
    dataset_id=dataset.id,
)\n
# Define your evaluator
def exact_match(run, example):
    return {"score": run.outputs["output"] == example.outputs["output"]}\n
experiment_results = evaluate(
    lambda input: "Welcome " + input['postfix'], # Your AI system goes here
    data=dataset_name, # The data to predict and grade over
    evaluators=[exact_match], # The evaluators to score the results
    experiment_prefix="sample-experiment", # The name of the experiment
    metadata={
      "version": "1.0.0",
      "revision_id": "beta"
    },
)
`,
    },
    typescript`
      import { Client, Run, Example } from "langsmith";
      import { evaluate } from "langsmith/evaluation";
      import { EvaluationResult } from "langsmith/evaluation";
      
      const client = new Client();
      
      // Define dataset: these are your test cases
      const datasetName = "Sample Dataset";
      const dataset = await client.createDataset(datasetName, {
        description: "A sample dataset in LangSmith.",
      });
      await client.createExamples({
        inputs: [
          { postfix: "to LangSmith" },
          { postfix: "to Evaluations in LangSmith" },
        ],
        outputs: [
          { output: "Welcome to LangSmith" },
          { output: "Welcome to Evaluations in LangSmith" },
        ],
        datasetId: dataset.id,
      });
      
      // Define your evaluator
      const exactMatch = async (
        run: Run,
        example: Example
      ): Promise<EvaluationResult> => {
        return {
          key: "exact_match",
          score: run.outputs?.output === example?.outputs?.output,
        };
      };
      
      await evaluate(
        (input: { postfix: string }) => ({ output: \`Welcome $\{input.postfix\}\` }),
        {
          data: datasetName,
          evaluators: [exactMatch],
          metadata: {
            version: "1.0.0",
            revision_id: "beta",
          },
        }
      );
    `,
  ]}
  groupId="client-language"
/>

- Learn more about evaluation in the [how-to guides](./how_to_guides/index.md).
