---
sidebar_label: Quick Start
sidebar_position: 1
table_of_contents: true
---

import Tabs from "@theme/Tabs";
import CodeBlock from "@theme/CodeBlock";
import {
  CodeTabs,
  typescript,
  python,
} from "@site/src/components/InstructionsWithCode";
import {
  LangChainInstallationCodeTabs,
  LangChainQuickStartCodeTabs,
  ConfigureEnvironmentCodeTabs,
  RunTreeQuickStartCodeTabs,
  ConfigureSDKEnvironmentCodeTabs,
  PythonSDKTracingCode,
  TypeScriptSDKTracingCode,
} from "@site/src/components/QuickStart";
import { ClientInstallationCodeTabs } from "@site/src/components/ClientInstallation";
import DocCardList from "@theme/DocCardList";
import { RegionalUrl } from "@site/src/components/RegionalUrls";

# Get started with LangSmith

**LangSmith** is a platform for building production-grade LLM applications.
It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.
LangChain's open source frameworks [langchain](https://python.langchain.com) and [langgraph](https://langchain-ai.github.io/langgraph/) work seemlessly with LangSmith but are not necessary - LangSmith works on its own!

## 1. Install LangSmith

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "bash",
      content: `pip install -U langsmith openai`,
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "bash",
      content: `yarn add langsmith openai`,
    },
  ]}
  groupId="client-language"
/>

## 2. Create an API key

To create an API key head to the <RegionalUrl text='Settings page' suffix='/settings' />. Then click **Create API Key.**

## 3. Set up your environment

<ConfigureSDKEnvironmentCodeTabs />

## 4. Log your first trace

:::tip LangSmith + LangChain OSS
You don't need to use the LangSmith SDK directly if your application is built on [LangChain](https://python.langchain.com)/[LangGraph](https://langchain-ai.github.io/langgraph/) (either Python and JS).

See the how-to guide for tracing with LangChain [here](./observability/how_to_guides/tracing/trace_with_langchain).
:::

We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use `traceable()`. See more on the [Annotate code for tracing](./observability/how_to_guides/tracing/annotate_code) page.

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "python",
      content: PythonSDKTracingCode(),
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "typescript",
      content: TypeScriptSDKTracingCode(),
    },
  ]}
  groupId="client-language"
/>

- View a [sample output trace](https://smith.langchain.com/public/b37ca9b1-60cd-4a2a-817e-3c4e4443fdc0/r).
- Learn more about tracing in the observability [tutorials](./observability/tutorials), [conceptual guide](./observability/concepts) and [how-to guides](./observability/how_to_guides/index.md).

## 5. Run your first evaluation

Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.

<CodeTabs
  tabs={[
    python`
        from langsmith import Client, traceable

        client = Client()

        # Define dataset: these are your test cases
        dataset = client.create_dataset(
            "Sample Dataset",
            description="A sample dataset in LangSmith.",
        )

        client.create_examples(
            inputs=[
                {"postfix": "to LangSmith"},
                {"postfix": "to Evaluations in LangSmith"},
            ],
            outputs=[
                {"response": "Welcome to LangSmith"},
                {"response": "Welcome to Evaluations in LangSmith"},
            ],
            dataset_id=dataset.id,
        )

        # Define an interface to your application (tracing optional)
        @traceable
        def dummy_app(inputs: dict) -> dict:
            return {"response": "Welcome " + inputs["postfix"]}

        # Define your evaluator(s)
        def exact_match(outputs: dict, reference_outputs: dict) -> bool:
            return outputs["response"] == reference_outputs["response"]

        # Run the evaluation
        experiment_results = client.evaluate(
            dummy_app, # Your AI system goes here
            data=dataset, # The data to predict and grade over
            evaluators=[exact_match], # The evaluators to score the results
            experiment_prefix="sample-experiment", # The name of the experiment
            metadata={"version": "1.0.0", "revision_id": "beta"}, # Metadata about the experiment
            max_concurrency=4,  # Add concurrency.
        )

        # Analyze the results via the UI or programmatically
        # If you have 'pandas' installed you can view the results as a
        # pandas DataFrame by uncommenting below:

        # experiment_results.to_pandas()

`,
    typescript`
import { Client } from "langsmith";
import { EvaluationResult, evaluate } from "langsmith/evaluation";

      const client = new Client();

      // Define dataset: these are your test cases
      const datasetName = "Sample Dataset";
      const dataset = await client.createDataset(datasetName, {
        description: "A sample dataset in LangSmith.",
      });
      await client.createExamples({
        inputs: [
          { postfix: "to LangSmith" },
          { postfix: "to Evaluations in LangSmith" },
        ],
        outputs: [
          { response: "Welcome to LangSmith" },
          { response: "Welcome to Evaluations in LangSmith" },
        ],
        datasetId: dataset.id,
      });

      // Define your evaluator(s)
      const exactMatch = async ({ outputs, referenceOutputs }: {
        outputs?: Record<string, any>;
        referenceOutputs?: Record<string, any>;
      }): Promise<EvaulationResult> => {
        return {
          key: "exact_match",
          score: outputs?.response === referenceOutputs?.response,
        };
      };

      // Run the evaluation
      const experimentResults = await evaluate(
        (inputs: { postfix: string }) => ({ response: \`Welcome $\{inputs.postfix\}\` }),
        {
          data: datasetName,
          evaluators: [exactMatch],
          metadata: { version: "1.0.0", revision_id: "beta" },
          maxConcurrency: 4,
        }
      );
    `,

]}
groupId="client-language"
/>

- Click the link printed out by your evaluation run to access the LangSmith experiments UI,
  and explore the results of your evaluation.
- Learn more about evaluation in the [tutorials](./evaluation/tutorials), [conceptual guide](./evaluation/concepts), and [how-to guides](./evaluation/how_to_guides/index.md).
