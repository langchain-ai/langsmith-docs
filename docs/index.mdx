---
sidebar_label: Quick Start
sidebar_position: 1
table_of_contents: true
---

import Tabs from "@theme/Tabs";
import CodeBlock from "@theme/CodeBlock";
import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
  typescript,
} from "@site/src/components/InstructionsWithCode";
import {
  LangChainInstallationCodeTabs,
  LangChainQuickStartCodeTabs,
  ConfigureEnvironmentCodeTabs,
  RunTreeQuickStartCodeTabs,
  ConfigureSDKEnvironmentCodeTabs,
  PythonSDKTracingCode,
  TypeScriptSDKTracingCode,
} from "@site/src/components/QuickStart";
import { ClientInstallationCodeTabs } from "@site/src/components/ClientInstallation";
import DocCardList from "@theme/DocCardList";
import { RegionalUrl } from "@site/src/components/RegionalUrls";

# Get started with LangSmith

**LangSmith** is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of [LangChain's open source frameworks](https://python.langchain.com) is not necessary - LangSmith works on its own!

## 1. Install LangSmith

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "bash",
      content: `pip install -U langsmith openai`,
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "bash",
      content: `yarn add langsmith openai`,
    },
  ]}
  groupId="client-language"
/>

## 2. Create an API key

To create an API key head to the <RegionalUrl text='Settings page' suffix='/settings' />. Then click **Create API Key.**

## 3. Set up your environment

<ConfigureSDKEnvironmentCodeTabs />

## 4. Log your first trace

:::tip LangSmith + LangChain OSS
You don't need to use the LangSmith SDK directly if your application is built on [LangChain](https://python.langchain.com)/[LangGraph](https://langchain-ai.github.io/langgraph/) (either Python and JS).

See the how-to guide for tracing with LangChain [here](./observability/how_to_guides/tracing/trace_with_langchain).
:::

We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use `traceable()`. See more on the [Annotate code for tracing](./observability/how_to_guides/tracing/annotate_code) page.

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "python",
      content: PythonSDKTracingCode(),
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "typescript",
      content: TypeScriptSDKTracingCode(),
    },
  ]}
  groupId="client-language"
/>

- View a [sample output trace](https://smith.langchain.com/public/b37ca9b1-60cd-4a2a-817e-3c4e4443fdc0/r).
- Learn more about tracing in the observability [tutorials](/docs/observability/tutorials), [conceptual guide](/docs/observability/concepts) and [how-to guides](./observability/how_to_guides/index.md).

## 5. Run your first evaluation

Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "python",
      content: `from langsmith import Client, evaluate
client = Client()\n
# Define dataset: these are your test cases
dataset_name = "Sample Dataset"
dataset = client.create_dataset(dataset_name, description="A sample dataset in LangSmith.")
client.create_examples(
    inputs=[
        {"postfix": "to LangSmith"},
        {"postfix": "to Evaluations in LangSmith"},
    ],
    outputs=[
        {"output": "Welcome to LangSmith"},
        {"output": "Welcome to Evaluations in LangSmith"},
    ],
    dataset_id=dataset.id,
)\n
# Define your evaluator
def exact_match(run, example):
    return {"score": run.outputs["output"] == example.outputs["output"]}\n
experiment_results = evaluate(
    lambda input: "Welcome " + input['postfix'], # Your AI system goes here
    data=dataset_name, # The data to predict and grade over
    evaluators=[exact_match], # The evaluators to score the results
    experiment_prefix="sample-experiment", # The name of the experiment
    metadata={
      "version": "1.0.0",
      "revision_id": "beta"
    },
)
`,
    },
    typescript`
      import { Client, Run, Example } from "langsmith";
      import { EvaluationResult, evaluate } from "langsmith/evaluation";
      
      const client = new Client();
      
      // Define dataset: these are your test cases
      const datasetName = "Sample Dataset";
      const dataset = await client.createDataset(datasetName, {
        description: "A sample dataset in LangSmith.",
      });
      await client.createExamples({
        inputs: [
          { postfix: "to LangSmith" },
          { postfix: "to Evaluations in LangSmith" },
        ],
        outputs: [
          { output: "Welcome to LangSmith" },
          { output: "Welcome to Evaluations in LangSmith" },
        ],
        datasetId: dataset.id,
      });
      
      // Define your evaluator
      const exactMatch = async (
        run: Run,
        example: Example
      ): Promise<EvaluationResult> => {
        return {
          key: "exact_match",
          score: run.outputs?.output === example?.outputs?.output,
        };
      };
      
      await evaluate(
        (input: { postfix: string }) => ({ output: \`Welcome $\{input.postfix\}\` }),
        {
          data: datasetName,
          evaluators: [exactMatch],
          metadata: {
            version: "1.0.0",
            revision_id: "beta",
          },
        }
      );
    `,
  ]}
  groupId="client-language"
/>

- Click the link printed out by your evaluation run to access the LangSmith experiments UI,
  and explore the results of your evaluation.
- Learn more about evaluation in the [tutorials](./evaluation/tutorials), [conceptual guide](./evaluation/concepts), and [how-to guides](./evaluation/how_to_guides/index.md).
