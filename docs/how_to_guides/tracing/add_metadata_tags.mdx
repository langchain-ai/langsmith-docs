---
sidebar_position: 5
---

import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
} from "@site/src/components/InstructionsWithCode";

# Add metadata and tags to traces

LangSmith supports sending arbitrary metadata and tags along with traces.

Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.

Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID.
For more information on tags and metadata, see the [Concepts](/concepts/tracing#tags) page. For information on how to query traces and runs by metadata and tags, see the [Filter traces in the application](/how_to_guides/monitoring/filter_traces_in_application) page.

<CodeTabs
  tabs={[
    PythonBlock(`import openai
import langsmith as ls
from langsmith.wrappers import wrap_openai
client = openai.Client()\n
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]\n
# highlight-next-line
# You can set metadata & tags **statically** when decorating a function
# Use the @traceable decorator with tags and metadata
# Ensure that the LANGCHAIN_TRACING_V2 environment variables are set for @traceable to work
@ls.traceable(
    run_type="llm",
    name="OpenAI Call Decorator",
    # highlight-next-line
    tags=["my-tag"],
    # highlight-next-line
    metadata={"my-key": "my-value"}
)
def call_openai(
    messages: list[dict], model: str = "gpt-3.5-turbo"
) -> str:
    # You can also dynamically set metadata on the parent run:
    # highlight-next-line
    rt = ls.get_current_run_tree()
    # highlight-next-line
    rt.metadata["some-conditional-key"] = "some-val"
    # highlight-next-line
    rt.tags.extend(["another-tag"])
    return client.chat.completions.create(
        model=model,
        messages=messages,
    ).choices[0].message.content\n
call_openai(
    messages,
    # To add at **invocation time**, when calling the function.
    # via the langsmith_extra parameter
    # highlight-next-line
    langsmith_extra={"tags": ["my-other-tag"], "metadata": {"my-other-key": "my-value"}}
)\n
# Alternatively, you can use the context manager
with ls.trace(
        name="OpenAI Call Trace",
        run_type="llm",
        inputs={"messages": messages},
        # highlight-next-line
        tags=["my-tag"],
        # highlight-next-line
        metadata={"my-key": "my-value"},
    ) as rt:
        chat_completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
        )
        rt.metadata["some-conditional-key"] = "some-val"
        rt.end(outputs={"output": chat_completion})
\n
# You can use the same techniques with the wrapped client
patched_client = wrap_openai(
    # highlight-next-line
    client, tracing_extra={"metadata": {"my-key": "my-value"}, "tags": ["a-tag"]}
)
chat_completion = patched_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages,
    langsmith_extra={
        # highlight-next-line
        "tags": ["my-other-tag"],
        # highlight-next-line
        "metadata": {"my-other-key": "my-value"},
    },
)
`),
    TypeScriptBlock(`import OpenAI from "openai";
import { traceable } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";
import { RunTree} from "langsmith";\n
const client = new OpenAI();\n
const messages = [
    {role: "system", content: "You are a helpful assistant."},
    {role: "user", content: "Hello!"}
];\n
const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {
    const completion = await client.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: messages,
    });
    return completion.choices[0].message.content;
},{
    run_type: "llm",
    name: "OpenAI Call Traceable",
    // highlight-next-line
    tags: ["my-tag"],
    // highlight-next-line
    metadata: { "my-key": "my-value" }
});
// Call the traceable function
await traceableCallOpenAI(messages, "gpt-3.5-turbo");\n
// Create a RunTree object
const rt = new RunTree({
  run_type: "llm",
  name: "OpenAI Call RunTree",
  inputs: { messages },
   // highlight-next-line
  tags: ["my-tag"],
   // highlight-next-line
  extra: { metadata: { "my-key": "my-value" } },
});
await rt.postRun();
const chatCompletion = await client.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: messages,
});
// End and submit the run
await rt.end(chatCompletion);
await rt.patchRun();`),
  ]}
  groupId="client-language"
/>
