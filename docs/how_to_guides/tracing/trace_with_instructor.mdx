---
sidebar_position: 16
---

# Trace with `Instructor` (Python only)

We provide a convenient integration with [Instructor](https://jxnl.github.io/instructor/), a popular open-source library for generating structured outputs with LLMs.

In order to use, you first need to set your LangSmith API key.

```shell
export LANGCHAIN_API_KEY=<your-api-key>
```

Next, you will need to install the LangSmith SDK:

```shell
pip install -U langsmith
```

Wrap your OpenAI client with `langsmith.wrappers.wrap_openai`

```python
from openai import OpenAI
from langsmith import wrappers

client = wrappers.wrap_openai(OpenAI())
```

After this, you can patch the wrapped OpenAI client using `instructor`:

```python
import instructor

client = instructor.patch(client)
```

Now, you can use `instructor` as you normally would, but now everything is logged to LangSmith!

```python
from pydantic import BaseModel


class UserDetail(BaseModel):
    name: str
    age: int


user = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=UserDetail,
    messages=[
        {"role": "user", "content": "Extract Jason is 25 years old"},
    ]
)
```

Oftentimes, you use `instructor` inside of other functions.
You can get nested traces by using this wrapped client and decorating those functions with `@traceable`.
Please see [this guide](./annotate_code) for more information on how to annotate your code for tracing with the `@traceable` decorator.

```python
# You can customize the run name with the `name` keyword argument
# highlight-next-line
@traceable(name="Extract User Details")
def my_function(text: str) -> UserDetail:
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": f"Extract {text}"},
        ]
    )


my_function("Jason is 25 years old")
```
